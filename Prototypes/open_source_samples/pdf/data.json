[
{"main_title": "Automotive industry in China"},
{"title": "History", "text_content": "The first automobile in China was purchased from Hong Kong in 1902 by Yuan Shikai and gifted to Empress Dowager Cixi.[6][7] It was later put on display in the Summer Palace Museum. During the early twentieth century, major western automobile manufacturers such as the Ford Motor Company,[8] General Motors,[9][10] and Mercedes-Benz[11] had plants operating in Shanghai. However, the Second Sino-Japanese War hampered the progress of the Chinese auto industry, as seen by the relocation of the Changan Automobile factory from Shanghai to Chongqing in the wake of the city's bombing and attack.[12] After the foundation of the People's Republic of China (PRC) in 1949, plants and licensed auto design were established in China with assistance from the Soviet Union in the 1950s, marking the beginning of the country's automobile sector. However, the Chinese automotive industry did not exceed 100–200 thousand automobiles produced per year during the first 30 years of the PRC.[13] China's annual automobile production capacity first exceeded one million in 1992. By 2000, China was producing over two million vehicles.[14] After China's entry into the World Trade Organization (WTO) in 2001, the development of the automobile market accelerated further. Between 2002 and 2007, China's national automobile market grew by an average 21 percent, or one million vehicles year-on-year.[15][16] In 2009, China produced 13.79 million automobiles, of which 8 million were passenger cars and 3.41 million were commercial vehicles and surpassed the United States as the world's largest automobile producer by volume. In 2010, both sales and production topped 18 million units, with 13.76 million passenger cars delivered, in each case the largest by any nation in history.[17] In 2017, total vehicle production in China reached 28.879 million, accounting for 30.19% of global automotive production.[18] In the first half of 2023, China overtook Japan to become the world's largest exporter of automobiles, exporting 2.34 million vehicles compared to 2.02 million for Japan.[19] As of 2024[update], China is the world's largest market both in terms of automobile sales and ownership.[20]: 105 The first Chinese-built motor vehicle was a truck called the Minsheng 75 truck (民生牌75). It was designed by Daniel F Myers, and a prototype was made at the Liao Ning Trench Mortar Arsenal, Shenyang. The prototype was completed on May 31, 1931, for Zhang Xueliang. Prior to production commencing, the factory was bombed during the Japanese invasion of Manchuria and production never commenced.[21] A fellow general, Yang Hucheng, patronized the inventor Tang Zhongming to make a new type of automobile engine powered by charcoal. In 1932 Tang founded the Chung Ming Machinery Co. Ltd. in Shanghai to produce the engines. Charcoal-powered vehicles were mainly used during the Second Sino-Japanese War because of fuel shortages.[22] Tung oil was also used during the war as a petroleum substitute.[23][24] The number of automobiles in China had been growing steadily which was close to 70,000 vehicles in 1937. However, due to the war, car ownership volume plummeted to 16,000 in 1940, which was only 23.8% of 1937. It was not until 1947 that car ownership volume returned to pre-war levels.[12] The development of the Chinese automobile industry following the Chinese Communist Revolution was relatively slow due to the lack of free market competition and the turbulence of the Culture Revolution. Except for a degree of development in the 1950s with assistance from the Soviet Union, the Chinese automobile industry remained closed and lagging behind until the period of Chinese economic reform under Deng Xiaoping. Most domestically produced vehicles were primarily the Jiefang trucks for military or industrial departments and the Hongqi sedans used by a limited number of government officials.[25] The concept of private cars had not yet emerged in China during this period.[26] Several vehicle assembly factories were set up in the 1950s and 1960s. They were Beijing (today's Beijing Automotive Industry Holding Corporation),[27] Shanghai (today's Shanghai Automotive Industry Corporation),[28] Nanjing (later Nanjing Automobile, merged with SAIC),[29] and Jinan (evolving into China National Heavy Duty Truck Group).[30] The Second Automobile Works (later Dongfeng Motor Corporation) was founded in 1968.[31]The first Chinese production vehicles were trucks made by the First Automotive Works in 1956, called the Jiefang CA-10.[32] This was followed on March 10, 1958, by the 2½ ton light duty truck (NJ130), which was based on the Russian GAZ-51, was produced in Nanjing. The truck was named Yuejin (meaning \"leap forward\") by China's First Ministry of Industrial Machinery.[25][33] In June 1958, the Nanjing Automobile, previously a vehicle servicing unit of the People's Liberation Army, began making China's first domestically produced light-duty trucks.[34] Production continued until the last truck (NJ134) rolled off the assembly line on July 9, 1987. Cumulative production was 161,988 units (including models NJ130, NJ230, NJ135, and NJ134). The first production automobiles were the Dongfeng CA71, Hongqi CA72, Feng Huang (later known as the Shanghai SH760) all from 1958.[35][36] Changan Automobile traces its origins back to 1862 when Li Hongzhang set up a military supply factory, the Shanghai Foreign Gun Bureau. It was not until 1979, when the factory was repurposed to manufacture Suzuki automobiles, that it became an automobile manufacturer.[37][38] The passenger car industry was a minor part of vehicle production during the first three decades of the People's Republic of China. As late as 1985, the country produced a total of only 5,200 cars.[39] Cars were almost entirely purchased by danweis (work units – private car ownership was virtually unknown at the time, in spite of the Sun Guiying story).[40] As domestic production was very limited, import totals rose dramatically despite a 260 percent import duty on foreign vehicles. Before 1984, the dominant exporter of cars to China had been the Soviet Union. In 1984, Japan's vehicle exports to China increased sevenfold (from 10,800 to 85,000), and by mid-1985, China had become Japan's second biggest export market after the U.S.[41] The country spent some $3 billion to import more than 350,000 vehicles (including 106,000 cars and 111,000 trucks) in 1985 alone. Three taxi companies in particular imported many Japanese cars such as Toyota Crowns and Nissan Bluebirds.[42] As this spending binge began to lead to a severe trade deficit, the Chinese leadership put on the brakes through the adjustment of import and foreign exchange policies.[43] Customs duties on imported goods were raised in March 1985, and a new \"regulatory tax\" was added a little later. In September 1985, a two-year moratorium on nearly all vehicle imports was imposed.[43] In July 1979, China adopted its first Law on Joint Venture Using Chinese and Foreign Investment. This law was effective in helping to attract and absorb foreign technology and capital from developed countries like the United States, facilitating China's exports to such countries and thereby contributing to China's subsequent rapid economic growth.[44] While limiting imports, China also tried to increase local production by boosting the various existing joint venture passenger car production agreements, as well as adding new ones. In 1983, American Motors Corporation (AMC, later acquired by Chrysler Corporation) signed a 20-year contract to produce their Jeep-model vehicles in Beijing. The following year, Germany's Volkswagen signed a 25-year contract to make passenger cars in Shanghai, and France's Peugeot agreed to another passenger car project to make vehicles in the prosperous southern city of Guangzhou.[42] These early joint ventures did not allow the Chinese to borrow much foreign technology, as knock-down kit assembly made up the majority of manufacturing activities;[45] tooling may not have been allowed to slip past borders. Until the late 1990s, there were eight joint venture enterprises in China producing passenger cars, including Shanghai Volkswagen, FAW-Volkswagen, Beijing Jeep, Guangzhou Peugeot, Dongfeng Citroën, Changan Suzuki, Changhe Suzuki, and Soueast Motor.[46] In April 1986, the Chinese government's seventh five-year plan, which recognized automobile manufacturing as a \"pillar industry\".[47][48] The Chinese automotive industry gradually moved away from the manual workshop model and adopted Western advanced technologies and quality control management. Over the course of a decade, the localization rate of Chinese automotive components significantly increased. In 1997, the localization rate of the SAIC-VW Santana, one of the most popular sedans in China at that time, jumped from 60.09% six years prior to over 90%, with key components like the car body, engine, transmission, and front and rear axle assemblies all achieving localization. The localization rate of the FAW-VW Audi 100 reached 93%, while the Jetta achieved an 84.02%. The localization rate of the Citroën Fukang by FAW exceeded 80%.[49][50] The improvement in the localization rates of complete vehicles were made possible by the growing capabilities of complementary enterprises in the industry chain. During this period, diesel engines from Yuchai Machinery Factory and automotive glass from Fuyao began to emerge.[51] Several enterprises entered the automobile industry beginning in 1994. Some of them are originated from the defense industry, such as Changan, Changhe, and Hafei;[52] some were developed from state-owned companies, such as BYD, Brilliance, Chery, and Changfeng Motor. Others are private-owned companies, such as Geely Auto and Great Wall Motor. China entered the World Trade Organization in 2001, marking a significant shift in the country's automotive industry. Following the admission, automotive tariffs began to be substantially reduced, leading to a decrease in the prices of imported cars. This reduction in tariffs transformed the market. As foreign automotive companies started bringing their latest models into China, Chinese consumers gained access to a wider variety of vehicles at more competitive prices, driving increased demand and competition within the industry.[53] By following WTO regulations, starting in 2006, the import tariffs on complete vehicles in China were lowered from the previous 30% to 28%. In 2010, they were further reduced to 25%. Tariffs on automotive components like transmissions, shock absorbers, radiators, clutches, and steering units decreased from 13.5% to 12.9% and eventually to 10%.[53] With China's entry into the WTO, competition from both domestic and foreign automotive brands increased. This intense competition caused prices in the domestic automotive market to decline steadily. The annual average reduction in car prices has exceeded 8%, with a particularly notable decrease of 13.5% in 2004.[54] The Chinese automotive market experienced significant growth after 2000. This growth is closely tied to China's economic development and the rise of the middle class. An increasing number of Chinese households can afford cars, leading to a surge in sales.[2] China's automobile production grew from two million vehicles in 2000 to 29 million vehicles in 2017. During that time, its global market share rose from 3% to 30%.[56] By 2017, there were 300.3 million registered vehicles in China.[57] In January 2007, China surpassed Japan to become the world's No. 2 vehicle market after the United States, with a 37 percent increase in car purchases.[58] An estimated 7.2 million vehicles was sold in China in 2006.[59] Following the 2007–2008 global financial crisis, the Chinese government implemented various policies to stimulate car purchases. This included a car-scrappage scheme and sales tax reductions on smaller vehicles, leading to a surge in demand for cars with engines less than 1.6 liters. Due to these stimulus measures, growth was particularly strong over 2009 and 2010, with production and sales of automobiles doubling over this period. Both the scrappage scheme and the sales tax discount ended in late 2010.[60] In 2010, the Chinese automotive industry became the largest in the world, surpassing the United States. Following a 59 percent year-on-year sales increase, China's car sales exceeded those of the US in 2009, with 13.6 million vehicles sold within the country compared to just over 10 million in the US.[61] At this point, almost 200 million Chinese people were able to drive a vehicle, making up about 15 percent of the country's 1.3 billion population.[62] With the rapid growth of China's automobile production, China became the country with the most diverse range of automotive brands globally. Competition in China's automobile market significantly intensified during this period.[63] However, the export market remained relatively small compared to the domestic market. In 2008, motor vehicle exports constituted about 7% of Chinese automobile production, decreasing to about 3% in 2009 due to the global financial crisis. Key export destinations in 2010 included Algeria, Vietnam, Russia, Iran, and Chile. Most motor vehicle exports at that time were directed towards developing and emerging economies.[60]Apart from mainstream joint venture brands dominating the mid-to-high-end market, there was a substantial presence of local state-owned and private small and medium-sized automotive companies. However, despite the Chinese government's policy of requiring foreign carmakers to establish local joint ventures, Chinese carmakers faced difficulty to compete with foreign competitors during this era. According to the China Association of Automobile Manufacturers (CAAM), local car brands saw their market share decline, dropping from 30.9 percent in 2010 to 26.8 percent by the end of July 2012. Experts attribute this lack of success to the joint ventures' failure to transfer know-how effectively. Former Chinese industry minister He Guangyuan likened auto manufacturing joint ventures to \"opium,\" criticizing Chinese firms for relying on assembling foreign cars with minimal changes instead of developing vehicles from scratch to gain know-how and patent rights.[65] To facilitate consolidation, in 2012, the government revoked production permits for manufacturers producing fewer than 1,000 passenger vehicles annually.[65] On February 29, 2016, the Ministry of Industry and Information Technology shut down 13 automobile manufacturers that did not meet mandatory production evaluations for two consecutive years.[66] After 2018, an increasing number of these smaller brands became 'zombie company' state, with many suspending production and operations, as market-driven consolidation accelerated. The number of Chinese automotive brands increased from just over 20 in the early 1990s to 84 in 2019.[67] In 2017, China imported $51 billion of vehicles.[68][69] In 2018, China lowered the vehicle import tariffs to 15%, and the vehicle components import tax to 6% to provide greater access for foreign automakers in China.[70] In 2009, the State Council of the People's Republic of China issued the \"Automobile Industry Adjustment and Revitalization Plan,\" which emphasized, \"Using new energy vehicles as a breakthrough, strengthening independent innovation to establish new competitive advantages.\"[71] It explicitly outlined China's plan to use electric vehicle.[71] This strategy is commonly referred to as the \"corner overtaking strategy\" in the Chinese automotive industry.[71] In 2010, China's sales of electric vehicles were only 5,000 units. By 2015, the sales had surged to 331,000 units. In 2015, the Xi Jinping Administration launched the Made in China 2025 industrial policy that prioritized electric vehicles.[72][73] By 2020, electric vehicles sales reached 1.367 million units, accounting for more than 50% of global market share.[74] Following the Chinese economic reform, from 1994 to 2018, Chinese automotive policy mandated that foreign carmakers had to establish joint ventures with a Chinese counterpart to produce vehicles in the country, with the Chinese partner owning at least 50% of the venture. This measure was implemented to protect local manufacturers and provide it with the chance to bridge the technology gap and develop their brands.[75] In the 2010s, automotive analysts speculated China would lift its restriction on joint venture ownership once the domestic industry matures.[76] In 2017, the Chinese government announced the intention to lift ownership restrictions in the automotive industry and allowed foreign automotive companies to take majority or full ownership of their operations in China.[77] On April 17, 2018, The National Development and Reform Commission (NDRC) of China announced that foreign ownership limits on automakers would be phased out over a 5-year period.[78][79][80] The goal of the Chinese government was to open the Chinese market to foreign companies and new technologies, ease trade tension, and increase market competition.[81] On 28 July 2018, China lifted foreign ownership restrictions on new energy vehicle production, which benefited American electric car manufacturer Tesla, Inc. The company established a plant in Shanghai, becoming the first foreign automaker to open a wholly-owned manufacturing facility in China.[83][84] The liberalization was followed by commercial vehicles in 2020 and passenger cars in 2022. The regulation preventing foreign automakers from forming more than two joint ventures in China was also lifted in 2022.[85] In December 2020, Volkswagen gained majority control of its Chinese electric car joint venture JAC-VW, controlling 75% of its Chinese business operation and renamed it to Volkswagen Anhui.[86] In 2021, Volvo took complete ownership control of its Chinese manufacturing and sales subsidiaries.[87] In 2022, BMW took control of its Chinese joint venture, BMW Brilliance with Brilliance Auto Group, reaching 75% of the stake.[88][89] Since 2020, the Chinese automotive industry has entered a phase marked by the maturation and advancement of technology among local manufacturers. As a result, there has been a notable increase in the market share held by local manufacturers within the domestic market. Additionally, many foreign brands have sought partnerships with Chinese automakers to capitalize on their technological advancements and supply chain capabilities.[90][91] According to the China Passenger Car Association (CPCA), in the first half of 2020, the market share of local brands in the Chinese automotive market was slightly more than 30 percent, with German and Japanese brands then at around 30 percent and 25 percent respectively. Two years later, in October 2022, the share of local car brands in China reached 51.53 percent. It was the first time in history that the monthly share of local car brands in China exceeded 50 percent. In contrast, the dominance of foreign brands are rapidly declining. The share of German brands fell to 19.25 percent, and Japanese brands fell to 18.94 percent in October 2022. Throughout 2023, the market share of local brands has remained at around 50 percent.[90][92] These changes were attributed to the rapidly increasing popularity of new energy vehicles, and the failure of foreign brands to catch up with the shift.[93] In 2024, the market share of foreign car brands fell to a record low of 37 percent.[94]Due to these market dynamics, some joint ventures that were already facing challenges during the era of traditional fuel-powered cars are further disadvantaged. In May 2023, Zhu Huarong, chairman of Changan Automobile, predicted that \"in the next 2–3 years, it is conservatively estimated that 60%-70% of brands will face closure and transfer.\"[95] Between 2018 and 2023, eight joint venture manufacturers opted to withdrew the Chinese market. Other joint ventures with significantly decreased sales are scaling back their production capacity by closing and selling their underutilized manufacturing plants. The remaining production capacity has been acquired by their Chinese joint venture partners.[95] In August 2023, BYD chairman and CEO Wang Chuanfu called on local Chinese car manufacturers to \"unite\" to take on foreign manufacturers, responding to the severe price war in the Chinese market throughout 2023. The call was welcomed by the CEOs of Nio and Li Auto.[96][97] Since late 2022, the Chinese automotive industry has experienced a significant price war characterized by aggressive price reductions by carmakers to attract customers and increase market share, amid an economic slowdown and production overcapacity.[4] Tesla initiated the subsequent price war by offering two substantial price cuts on its Chinese-made models in October 2022 and January 2023. The situation was also caused by the fact that China's automobile industry is moving towards electrification, which led to overcapacity of internal combustion engine vehicles.[98] In 2023, China's light vehicle production capacity was 48.7 million units, with a capacity utilization rate of 59%.[98] By 2023, Reuters reported that over 40 carmakers in China in both internal combustion engine and electric vehicle segments followed suit to maintain their market position. Brands resorted to extreme measures by offering deep discounts and other incentives while pressing auto suppliers to reduce costs. The competitive climate also caused a heightened focus on innovation and value-added features in vehicles.[99] However, there are concerns from analysts, journalists and executives in the industry about its long-term effects on the overall health and stability of the Chinese automotive industry.[100][101] In mid-2023, Bloomberg News reported most top Chinese automakers, except BYD and Changan, suffered a decline in profits as a result of the price war, hitting its lowest since the beginning of the COVID-19 pandemic in 2020. BYD, which specialized in electric vehicles, became an outlier as it experienced record profits and deliveries in this period, securing its position as a key player in the market.[102] The market dynamics also drove the share of Chinese automakers to an all-time high, accounting for slightly over 50% of the market.[103][104] However, the market dynamics in China also led to overcapacity, especially in EVs, which prompted Chinese carmakers to increase exports and expand sales overseas.[105][4] Declining sales and profits also affected foreign joint venture brands.[106] In March 2023, SAIC-Volkswagen reduced prices on its ID.3 electric cars by 18 percent.[103] Toyota implemented workforce reductions at GAC Toyota, eliminating around 1,000 jobs. Additionally, Hyundai sold two of its plants, while Mitsubishi Motors left the market completely in that year.[102] Layoffs were also observed at GAC Honda, Volkswagen, Volvo, Tesla, and Kia.[104] The Chinese government has attempted to mitigate the negative impacts of the price war through various measures, such as subsidies for electric vehicle purchases and initiatives to promote the adoption of new energy vehicles in rural areas.[107] In July 2023, sixteen manufacturers, including fifteen Chinese carmakers and Tesla, signed an agreement facilitated by the China Association of Automobile Manufacturers (CAAM) to avoid \"abnormal pricing\" practices and prevent a price war.[108] However, just two days later, CAAM retracted the \"abnormal pricing\" clause due to concerns about violating China's antitrust laws. This move quickly ended the temporary \"peace\" and triggered another round of price cuts.[103][109] In the 2020s, foreign global manufacturers started seeking technological assistance from its Chinese counterparts and invested in China through joint ventures or other forms of partnerships,[91] including Renault-Nissan, VW, BMW, Mercedes-Benz, Toyota, Stellantis, and Jaguar Land Rover. Since the 2020s, Chinese technology corporations such as Huawei, Baidu, DJI have entered the automotive business. Huawei's partnership with automobile manufacturers has taken the form of three business models, the standardized parts supply model, the \"Huawei Inside\" (HI) model, and the Harmony Intelligent Mobility Alliance (HIMA).[126][127] Baidu and DJI have provided autonomous driving system and hardware to automotive manufacturers.[128][129] Qihoo 360 invested in the Chinese EV startup company Hozon Auto.[130][better source needed] Geely collaborates with Baidu to set up joint venture brands, and acquired Chinese smartphone company Meizu for its Polestar and Lynk & Co brands with its auto OS and AR system. Xiaomi is the first and the only Chinese tech company that is directly involved in automotive design, development and manufacturing, and operates its factory in Beijing.[131]", "images": ["//upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Jiefang_CA10_at_PRC70_Exhibition_%2820191203152134%29.jpg/200px-Jiefang_CA10_at_PRC70_Exhibition_%2820191203152134%29.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Hongqi_CA72_1959_%2814669521938%29.jpg/200px-Hongqi_CA72_1959_%2814669521938%29.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/b/ba/1964_Shanghai_SH760_--_Shanghai_Automobile_Museum_--_2012-05-26.jpg/200px-1964_Shanghai_SH760_--_Shanghai_Automobile_Museum_--_2012-05-26.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/1/18/Traffic_circle_with_pedestrian_overcrossing%2C_China%2C_1987.jpg/220px-Traffic_circle_with_pedestrian_overcrossing%2C_China%2C_1987.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/6/64/Original_Beijing_Jeep_XJ_Cherokee.jpg/200px-Original_Beijing_Jeep_XJ_Cherokee.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/9/98/Chinese_VW_Santana_sedan2.jpg/200px-Chinese_VW_Santana_sedan2.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/6/62/2003_Geely_Haoqing_%28HQ%29_JL6360E1%2C_front_8.12.18.jpg/200px-2003_Geely_Haoqing_%28HQ%29_JL6360E1%2C_front_8.12.18.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Wuling_Hongguang_Sanming_01_2022-07-01.jpg/200px-Wuling_Hongguang_Sanming_01_2022-07-01.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Haval_H6_II_%28cropped%29.jpg/200px-Haval_H6_II_%28cropped%29.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Tesla_Gigafactory_Shanghai_aerial_view_03.png/220px-Tesla_Gigafactory_Shanghai_aerial_view_03.png", "//upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Li_Auto_Beijing_Manufacturing_Base_%2820240329140107%29.jpg/220px-Li_Auto_Beijing_Manufacturing_Base_%2820240329140107%29.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Tesla_Model_Y_2021032001.jpg/220px-Tesla_Model_Y_2021032001.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/7/78/Smart_1_003.jpg/200px-Smart_1_003.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/7/70/Mini_Hatch_%28J01%29_Cooper_S_IAA_2023_1X7A0724.jpg/200px-Mini_Hatch_%28J01%29_Cooper_S_IAA_2023_1X7A0724.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/9/96/AITO_M9_in_Grandview_Mall%2C_Guangzhou_20240214-A.jpg/220px-AITO_M9_in_Grandview_Mall%2C_Guangzhou_20240214-A.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/7/74/Xiaomi_SU7_Max_001.jpg/220px-Xiaomi_SU7_Max_001.jpg"], "table": []},
{"title": "Supply chain", "text_content": "In terms of electric vehicle production, China has a significant advantage over other countries. The Chinese automotive industry holds a dominant position in the electric vehicle supply chain, with more than 600,000 EV-related enterprises operating in the country as of 2022[update].[132][unreliable source?] Chinese manufacturers' share of the global EV battery market stood at 60% in 2022.[105][133] Industry analyst Chris Berry stated that China has a 10 to 15-year head start on the rest of the world in terms of EV battery supply chain.[134] The dominance of the EV battery supply chain is considered a major factor contributing to the lower cost of Chinese EVs. Some 75 percent of the world's lithium-ion batteries are made in China, and the country's EV manufacturing facilities are close to the source of these components. China has invested heavily in refinery capacity, housing more than half of the world's processing and refining capacity for lithium, cobalt, and graphite, which are essential materials for making EV batteries. 70 percent of the global production capacity for cathodes and 85 percent for anodes are also hosted in China.[132][unreliable source?] China's strength in EV supply chain resulted in reduced costs in logistics, labor, and land management. Additionally, economies of scale are enabled by its large domestic EV market. China's EV manufacturing sector enjoys a cost advantage of 20 percent compared to Western markets such as those in the U.S. and Europe.[132][unreliable source?] In January 2023, according to an executive of French automotive supplier Forvia, Chinese carmakers can build an electric vehicle (EV) for €10,000 less than European carmakers, an overwhelming cost advantage that will put pressure on European manufacturers in their home market. Chinese manufacturers are able to produce electric vehicles at lower cost by having lower research and development costs, lower levels of capital spending, and lower labor costs than European rivals.[135] The entry of Tesla to the Chinese market has greatly benefited China's automotive supply chain. The company has been responsible for imposing the \"catfish effect\"[jargon] on the Chinese EV industry, which forced Chinese manufacturers to innovate and match with Tesla from technology advancement to affordability.[133]", "images": [], "table": []},
{"title": "Technological innovation", "text_content": "Amidst the fierce domestic competition in China's domestic market, Chinese automakers have established the building blocks for growing competitiveness in EV technology, software, digitalization, factor cost and supply chain areas.[136] China's domestic brands lead the market in the development and implementation of advanced assisted driving systems, capitalizing on their early-entry advantages in the electric and intelligent vehicle sector.[137] According to investment bank Goldman Sachs, newly opened Chinese car plants are the most robotized of such facilities worldwide.[138]", "images": ["//upload.wikimedia.org/wikipedia/commons/thumb/7/79/NIO_Power_Battery_Swap.jpg/220px-NIO_Power_Battery_Swap.jpg"], "table": []},
{"title": "Sales and marketing", "text_content": "In China, authorized car dealerships are called 4S car shops. The 4S represents sales (整车销售), spare parts (零配件), service (售后服务) and survey (信息反馈). In most cases, brand-name new cars can only be purchased from 4S shops. The profit of car dealerships in China is quite high compared to the rest of the world, in most cases 10%.[citation needed] This is supposedly due to the 'non-transparent invoice price' as announced by manufacturers and to the premiums they charge for quick delivery. Due to the lack of knowledge for most customers, dealers can sell add-ons at much higher prices than the aftermarket. For new cars in high demand, a high premium is added for instant delivery or just placing an order. There is no regulation by either the government or associations, but some retailers are members of the China Automobile Dealers Association (CADA).[139] Direct sales are allowed in China, and have gained popularity in the 2020s, driven by new energy vehicle brands. Many electric car brands such as Nio, XPeng and Huawei's HIMA rely heavily or solely on the direct sales model. Traditional automakers have also started adopting this sales model. This phenomenon has led to a reduction in the number of traditional dealerships.[140] Car brand and model names in China typically include both an English name and a Chinese name chosen by the manufacturer, often sounding different or unrelated to the English name, regardless of whether they are from a foreign or domestic brand.[141][142] For example, the Chinese name of Toyota, \"丰田\"; Fēngtián sounds different from its original name, however the same kanji characters in Japanese means \"Toyota\". On the other hand, Mazda chose to use an identical-sounding transliterated name, \"马自达\"; Mǎzìdá.[143] Another example is AITO that has a completely unrelated Chinese name by character, sound or meaning, which is \"问界\"; Wènjiè, with literal meaning 'ask the world'.[144] Another Chinese automotive market phenomenon is the requirement for a manufacturer name badging in Chinese characters on the rear of every locally produced vehicle. The badging is mandated by the \"Measures for the Administration of External Markings of Automotive Products\" implemented by the Chinese government in February 2006, which specifically requires manufacturers to write the names of automobile manufacturers in Chinese characters in a specific size and material. Its purpose is to highlight the vehicle's \"Made in China\" status. This regulation does not apply to imported vehicles or exported vehicles.[145]", "images": ["//upload.wikimedia.org/wikipedia/commons/thumb/c/c1/20210627_Zhengzhou_Lei_Shing_Mercedes-Benz_dealership.jpg/220px-20210627_Zhengzhou_Lei_Shing_Mercedes-Benz_dealership.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/SZ_%E6%B7%B1%E5%9C%B3_Shenzhen_%E7%A6%8F%E7%94%B0_Futian_%E7%9A%87%E5%BA%AD%E5%BB%A3%E5%A0%B4_Wongtee_Plaza_%E5%95%86%E5%A0%B4_Mall_shop_LYNK_%26_Co_car_showroom_January_2024_R12S_01.jpg/220px-SZ_%E6%B7%B1%E5%9C%B3_Shenzhen_%E7%A6%8F%E7%94%B0_Futian_%E7%9A%87%E5%BA%AD%E5%BB%A3%E5%A0%B4_Wongtee_Plaza_%E5%95%86%E5%A0%B4_Mall_shop_LYNK_%26_Co_car_showroom_January_2024_R12S_01.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/d/dd/TOYOTA_CAMRY_HYBRID_%28XV70%29_China_%282%29_%28cropped%29.jpg/220px-TOYOTA_CAMRY_HYBRID_%28XV70%29_China_%282%29_%28cropped%29.jpg"], "table": []},
{"title": "Green vehicles", "text_content": "China encourages the development of clean and fuel-efficient vehicles in an effort to sustain the continued growth of the country's automobile industry (see Fuel economy in automobiles). By the end of 2007, China plans to reduce the average fuel consumption per 100 km for all types of vehicles by 10%. The proportion of vehicles burning alternate fuel will be increased to help optimize the country's energy consumption. Priority has been given to facilitating the research and development of electric and hybrid vehicles as well as alternative fuel vehicles, especially CNG/LNG.[146][147] On March 10, 2008, Beijing became the first city to require light-duty vehicles to meet the China-4 emission standard, which was equivalent to Euro-4. Beijing shifted its emission standards to the fifth-stage standards for light-duty and heavy-duty vehicles in January 2013 and August 2015, respectively. On 12 April 2016, the Ministry of Environmental Protection (MEP) released the proposal for the light-duty China-6 standard.[146] Due to serious air pollution problems and ever-increasing traffic, alternative-energy vehicle production is an area of strong focus for the Chinese government, and several electric vehicle-friendly policies have appeared at the national and local levels as a result. In many cities, free licenses — otherwise a significant expenditure for traditional vehicles — are provided for electric vehicle owners, along with exemptions for registry lotteries. These policies have created a strong interest in new energy vehicles in China.[148][better source needed] As of December 2015[update], China is the world's largest electric bus market, and by 2020, the country was expected to account for more than 50% of the global electric bus market.[149] China also is the world's leader in the plug-in heavy-duty segment, including electric buses, plug-in trucks, particularly sanitation/garbage trucks.[150][151] The government was encouraging the purchase of such cars with a short wait time for a new license plate and with government-backed discounts of up to 40% on electric vehicles.[163] In 2018, new-energy vehicles accounted for about 3% of China's new car sales.[164] In October 2018, Tesla purchased land for the construction of an EV manufacturing plant in Shanghai's Lingang area.[165][166] By then, VW had already begun construction of its EV factory, with a planned annual capacity of 300,000 SAIC-VW MEB platform vehicles.[167][168][164] As of 2022[update], major electric vehicle players in the Chinese industry include BYD Auto, Tesla China, SAIC-GM-Wuling, GAC Aion, and Changan Automobile. These five companies held more than 50 percent market share combined.[132] Chinese brands also account for about half of all EVs sold globally.[169][needs update] The Chinese Automotive Industry Plan, announced on the main website of China's central government, said China aims to create capacity to produce 500,000 new energy vehicles, such as battery electric cars and plug-in hybrid vehicles. The plan aims to increase sales of such new-energy cars to account for about 5% of China's passenger vehicle sales.[170] At the 2010 Beijing Motor Show, more than 20 electric vehicles were on display, most of which came from native automakers. As of May 2010, at least 10 all-electric models have been reported to be on track for volume production.[171] In 2009, the Chinese government implemented policies to subsidize the purchase of plug-in hybrid and electric cars and buses in 10 cities. The per unit subsidies for passenger cars ranged between RMB 4,000 to RMB 60,000. In ten major cities such as Beijing and Xi’an, Chinese EV producers worked closely with taxi companies to formulate operational solutions that would improve core battery technologies, such as implementing multiple shifts.[172] On November 2, 2020, the Chinese government introduced the \"New Energy Vehicle Industry Development Plan (2021–2035)\" to achieve a sustainable automotive future with reduced emissions. This plan is part of supportive policies aimed at strengthening the EV industry. On 21 June 2023, China unveiled a significant RMB 520 billion (US$72.3 billion) tax incentive package spanning four years to provide tax breaks for new energy vehicles. It offers a complete exemption from purchase tax for electric vehicles bought in 2024 and 2025, resulting in potential savings of up to RMB 30,000 (US$4,170) per vehicle. From 2026 to 2027, the exemption will be halved and capped at RMB 15,000 (US$2,078). This initiative aims to stimulate automotive industry growth amidst sluggish auto sales. Regions like Shenzhen and Shanghai have also introduced local initiatives to support the electric vehicle industry, including financial support and implementation plans to drive growth in their respective regions.[132][unreliable source?]", "images": ["//upload.wikimedia.org/wikipedia/commons/thumb/9/96/Roewe_ERX5_EV_charging.jpg/220px-Roewe_ERX5_EV_charging.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/d/d3/NEV_cum_sales_China_from_2011.png/325px-NEV_cum_sales_China_from_2011.png"], "table": []},
{"title": "Exports", "text_content": "In 2012, exports of Chinese automobiles were about 1 million vehicles per year and mostly to emerging markets.[174] By 2022, Chinese car exports reached 3.11 million units, ranking second worldwide. Domestic sales still accounted for the bulk of the 27 million units produced. Electric cars sales totaled 679,000.[175] In 2023, China overtook Japan, becoming the largest car exporter in the world. The increased export numbers contributed to the growing demand for electric cars.[3] Unlike local Chinese manufacturers, joint venture manufacturers were reluctant to export their vehicles from China due to having to share 50% of the profit with its local partner, as opposed to keeping a full profit by exporting from fully-owned plants elsewhere.[176] Notable exceptions in the early era included Honda, which formed China Honda Automobile in 2003 to produce vehicles for exports to Europe,[177] and SAIC-VW that exported Volkswagen Polo to Australia in 2004.[178][better source needed] As a result of excess production capacity, low cost of production, and the more accessible electric car supply chain,[179][176] some joint ventures such as SAIC-GM,[180] Changan Ford and Jiangling Motors (since 2018),[181] Beijing Hyundai (since 2018),[182] Yueda Kia (since 2018),[176] Dongfeng Honda and GAC Honda (since 2023),[183] and others started shipping vehicles from China to overseas markets. According to a report from McKinsey, while Chinese car companies have performed well in overseas markets in recent years, their operating model remains based on \"pure export,\" making them less mature when compared to international car companies that have been deeply involved in overseas markets for many years.[citation needed] For example, only around 40% of Japanese vehicle manufacturers' sales are produced in Japan, while 60% are produced and sold in overseas markets it operates in.[184] China's Belt and Road Initiative (BRI) gave impetus to the country's automotive industry, as BRI member countries have tended to receive almost double the Chinese automobile exports when compared to non-BRI member countries.[185] As of at least 2024, the Chinese EV industry is in a strong competitive position in the developing world market, including Southeast Asia.[186]: 58–59 .mw-parser-output .citation{word-wrap:break-word}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}^ The figures of SAIC includes the SAIC-GM and SAIC-GM-Wuling", "images": ["//upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Haval_Motor_Manufacturing_Rus_02.jpg/220px-Haval_Motor_Manufacturing_Rus_02.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/e/ee/2022_MG_ZS_Exclusive_T-GDi_1.0_Front.jpg/220px-2022_MG_ZS_Exclusive_T-GDi_1.0_Front.jpg", "//upload.wikimedia.org/wikipedia/commons/thumb/1/19/SAIC_Anji_at_Tianjin_May_2024.png/220px-SAIC_Anji_at_Tianjin_May_2024.png"], "table": [{"Year": ["2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023"], "Total": ["544,900", "814,000", "1,056,100", "977,300", "910,400", "699,400", "708,000", "891,000", "1,041,000", "1,024,000", "995,000", "2,015,000", "3,111,000", "5,220,000"], "Passenger vehicle": ["283,000", "476,100", "661,200", "596,300", "533,000", "345,400", "477,000", "639,000", "758,000", "725,000", "760,000", "1,614,000", "2,529,000", "4,450,000"], "Commercial vehicle": ["261,900", "338,200", "394,900", "381,000", "377,300", "354,000", "231,000", "252,000", "283,000", "299,000", "235,000", "402,000", "582,000", "770,000"]}, {"Rank": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], "2023": ["SAIC", "Chery", "Geely", "Changan", "Tesla", "GWM", "BYD", "Dongfeng", "BAIC", "JAC"], "2022": ["1,090,000", "925,000", "408,000", "358,000", "344,000", "316,000", "252,000", "231,000", "190,000", "170,000"], "2021": ["SAIC", "Chery", "Tesla", "Changan", "Dongfeng", "Geely", "GWM", "JAC", "BAIC", "Sinotruk"], "2020": ["906,000", "452,000", "271,000", "249,000", "242,000", "198,000", "173,000", "115,000", "110,000", "83,000"], "2019": ["SAIC", "Chery", "Tesla", "Changan", "Dongfeng", "GWM", "Geely", "BAIC", "JAC", "Sinotruk"], "2018": ["598,000", "269,000", "163,000", "159,000", "154,000", "143,000", "115,000", "81,000", "74,000", "54,000"]}]},
{"title": "Foreign tariffs and restrictions", "text_content": "During the 2020s, the export of Chinese-built automobiles has notably increased. However, their presence abroad has led to heightened tariffs and restrictions, attributed to allegations such as dumping, state subsidies, production overcapacity, national security, and forced labor. Critics argue that such allegations are a justification for protectionism.[190][191][192][193] In response to forced technology transfer allegations, the U.S. launched a probe in 2017 under Section 301 of the Trade Act of 1974.[194] During the first presidency of Donald Trump, the U.S. imposed a stiff 27.5 percent tariff for Chinese-made cars and has buttressed that with the protectionist tax credits of President Joe Biden's Inflation Reduction Act, which incentivized electric car and battery production in North America. In addition, hostility toward China from leaders in both political parties of U.S. make it difficult for Chinese carmakers to penetrate the U.S. market.[195] In November 2023, the United States House Select Committee on Strategic Competition between the United States and the Chinese Communist Party asked the Office of the United States Trade Representative to further hike tariffs on Chinese-made vehicles and investigate ways to prevent Chinese companies from exporting to the United States from Mexico to protect the U.S. automobile industry.[196] In December 2023, the U.S. government rolled out rules for electric vehicle tax credits so that any car using parts that comes from company which has more than 25 percent of board seats controlled by China will be disqualified from a US$7,500 subsidy.[197][198] In February 2024, the U.S. government blocked the import of several models of Volkswagen vehicles under the Uyghur Forced Labor Prevention Act, accusing some component of them were produced under forced labor in Xinjiang.[199][200] Volkswagen previously denied the accusations, claiming that they could not find any indications or evidence of forced labor among the employees through a third party audit.[201][202] In April 2024, when the U.S. Treasury Secretary Janet Yellen visited China, she accused the Chinese automotive industry of having overcapacity and tilting the playing field away from American workers and firms.[192][203] While some industry observers consider that the issue of overcapacity raised by U.S. is an excuse for protectionism.[193] In May 2024, the U.S. Commerce Secretary Gina Raimondo said the U.S. could take \"extreme action\" to ban Chinese vehicles or impose restrictions on them for national security reasons.[190] U.S. President Joe Biden later unveiled a hike in tariffs on Chinese-made EVs, quadrupling the duties from 25 percent to over 100 percent.[191] The International Monetary Fund criticized the Biden administration's decision to raise tariffs on Chinese goods, including EVs, urging the U.S. to maintain open trade policies.[204] In August 2024, Canada announced a 100% tariff on imported Chinese electric vehicles in addition to other tariffs.[205] In September 2023, European Commission President Ursula von der Leyen announced EU would launch an anti-subsidy investigation into Chinese electric vehicle manufacturers. Von der Leyen claims that the global markets are \"flooded\" with cheaper Chinese electric cars, and their price is kept artificially low by significant state subsidies that distort the EU market.[206] Chinese newspaper People's Daily stated that the investigation proposed by the EU is a practice of protectionism in the name of \"fair competition.\"[207] Carlos Tavares, the CEO of Stellantis criticized the investigation, stating it is not the optimal approach to global trade issues. He stressed the need for a global perspective to address challenges and promote competition and urged European politicians to support the region's automakers in competing with Chinese rivals offering competitively priced vehicles.[208][209] Chinese companies have been able sell cars at significantly higher prices with larger profit margins in the EU than in the Chinese domestic market.[210] According to research group Rhodium Group, European duties of around 45 to 55 percent would be needed to render exports to the European market unappealing.[210] Following the EU's anti-subsidy investigation, in June 2024, the European Commission (EC) announced new tariffs for Chinese-built electric vehicles (on top of an existing 10 percent tariff for all foreign-made vehicles regardless of engine type), which went into effect on 4 July 2024.[211] While analysts had variously predicted tariffs of between 10 and 25 percent, the EC would impose tariffs up to 38.1 percent. Electric vehicles made by BYD would face a 17.4 percent import duty, vehicles from Geely will be subject to a 20 percent duty, and vehicles from state-owned SAIC Motor would be subjected to the highest tariff of 38.1 percent. Manufacturers that neither received inspections nor provided information would face the maximum duty of 38.1 percent, while those that cooperated would be charged 21 percent.[212] On 26 June, after receiving more information from the affected companies, the EU reduced the proposed tariffs from 38.1 percent to 37.6 percent for SAIC, and 20 percent to 19.9 percent for Geely.[213][214] China's Ministry of Commerce criticized the EU for \"ignoring\" facts, WTO regulations, objections from China, and appeals from various EU member states and industries. German Chancellor Olaf Scholz cautioned against limiting automotive trade with China, emphasizing the importance of keeping markets open. German automakers such as Volkswagen and BMW, who collectively sold 4.6 million cars in China in 2022, would be significantly impacted by trade tensions. Western manufacturers, including Mercedes-Benz, have opposed the tariffs, with concerns about market openness. Mercedes-Benz faces vulnerability as the Chinese market is its primary export market. Volkswagen said the decision's timing is seen as unfavorable for electric vehicle demand, raising concerns about potential trade conflict escalation.[215] In July 2024, SAIC Motor issued a statement stating that it would formally request the European Commission to hold a hearing on the anti-subsidy investigation. The company claimed that the European Commission's investigation asked SAIC to disclose its commercially sensitive information including battery-related chemical formulas, which SAIC declined as it is beyond the scope of a normal investigation.[216][217][218] In September 2024, the EU rejected offers from Chinese electric vehicle makers for minimum import prices.[219] In October 2024, EU leaders approved additional tariffs on Chinese EVs, despite opposition from five countries, including Germany, which warned the decision could harm its auto industry. The European Commission, having provisionally backed the tariffs after finding unfair state aid to Chinese manufacturers, was set to impose duties of up to 35.3% for five years starting in November 2024. While ten member states, including France and Italy, supported the tariffs, Germany and Hungary opposed them, citing potential damage to local carmakers. The decision sparked concerns of a trade war with China, which condemned the move as protectionist.[220] China launched a WTO complaint in response.[221] In November 2024, the Financial Times reported that the EU is planning to require Chinese automotive companies to transfer technology to European businesses in return for EU subsidies.[222] The proposal triggered widespread criticism among Chinese automotive industry.[223][224][225] In June 2024, Turkey implemented a 40 percent additional tariff or a US$7,000 minimum tariff, whichever is higher, on vehicle imports from China, effective July 7, 2024. This decision follows Turkey's introduction of additional tariffs on Chinese electric vehicle imports in 2023.[226] The rationale behind Turkey's policy is to safeguard domestic vehicle production and reduce the current account deficit.[227] Chinese automobile brands such as Chery are considering setting up production facilities in Turkey to circumvent the tariffs.[228] In July 2024, Turkey announced that companies which invested in Turkey would be exempt from the new tariffs.[229] India has been proactive in rejecting investment plans from Chinese car manufacturers due to the Sino-Indian border dispute and a tougher stance towards investments from China.[230] Great Wall Motor initially proposed an investment of US$1 billion and had plans to start manufacturing in 2021 by buying a former General Motors plant, before canceling its plans in July 2022 due to failure of obtaining regulatory approvals. In July 2023, BYD Auto was forced to cancel its investment plans worth US$1 billion to produce cars in India due to scrutiny from the Indian government, noting \"security concerns\", despite 16-year presence of BYD Company in the country producing electronics and electric buses.[231] MG Motor India had struggled to receive clearance from the Indian Government to obtain capital from parent SAIC Motor until a local company JSW Group acquired a 35% share in the company.[232]", "images": [], "table": []},
{"title": "Criticism", "text_content": "In the 2010s, allegations of forced technology transfer arose in the Western automotive sector and beyond. The criticism centered around the government's joint venture policies, which required technology transfer in exchange for access to the country's sizable domestic market.[233][234][235][194][236] Criticism grew following the government's eleventh five-year plan, which adopted a more focused approach to technology transfer in advanced technology.[237] In 2010, foreign automakers complained about a Ministry of Industry and Information Technology plan which they said compelled sharing of critical technologies in electric vehicles.[233] According to The New York Times, General Motors was asked to disclose key technological information on the Volt.[235] Steve Girsky, the Vice Chairman of General Motors, told reporters that neither SAIC nor the Chinese government have requested Volt technology.[238] The Chinese government has consistently denied allegations of impropriety, stated that technology transfer is in line with WTO rules.[234][239] In 2017, the Ministry of Commerce stated that the establishment of joint ventures by foreign companies in China is a voluntary behavior and that there is no law in China that forces foreign investors to transfer technology.[240] In 2019, in an effort to attract additional foreign investors and respond to criticism, the National People's Congress passed a law making forced technology transfers illegal.[241] German economist Daniel Gros suggested that costs to Western companies imposed by technology transfer are \"vastly overstated.\" He also stated that increasing royalties payments to foreign automakers suggests that a \"large and growing share\" of technology transfer is not forced.[242] Yu Yongding, a member of the Chinese Academy of Social Sciences, said that foreign companies clearly understand what benefits they can get through partnerships with Chinese companies, which means such cooperation is mutually beneficial.[243] Jim Saker, president of the Institute of the Motor Industry in the UK, describes Chinese cars in the UK as \"invasion by trojan horse\" and alleges there are \"major security issues\" with Chinese cars. He claimed there was \"no way\" of preventing these vehicles being disabled remotely by car companies in China. No evidence was provided by Saker to substantiate his claims, with other experts dismissing them as scaremongering.[244]", "images": [], "table": []},
{"title": "See also", "text_content": "", "images": [], "table": []},
{"title": "References", "text_content": "", "images": [], "table": []},
{"main_title": "Large language model"},
{"title": "History", "text_content": "Before 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time.[4] In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\"web as corpus\"[5]), upon which they trained statistical language models.[6][7] In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.[8] After neural networks became dominant in image processing around 2012,[9] they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. As it was before transformers, it was done by seq2seq deep LSTM networks.At the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper's goal was to improve upon 2014 seq2seq technology,[10] and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014.[11] The following year in 2018, BERT was introduced and quickly became \"ubiquitous\".[12] Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.[13] Although decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use.[14] GPT-3 in 2020 went a step further and as of 2024[update] is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz.[15] The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities.[16] OpenAI did not reveal the high-level architecture and the number of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work.[17] Competing language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.[18] Since 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of June 2024[update], The Instruction fine tuned variant of the Llama 3 70 billion parameter model is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.[19] Since 2023, many LLMs have been trained to be multimodal, having the ability to also process or generate other types of data, such as images or audio. These LLMs are also called large multimodal models (LMMs).[20] As of 2024, the largest and most capable models are all based on the transformer architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).[21][22][23]", "images": ["//upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Trends_in_AI_training_FLOP_over_time_%282010-2025%29.svg/220px-Trends_in_AI_training_FLOP_over_time_%282010-2025%29.svg.png", "//upload.wikimedia.org/wikipedia/commons/thumb/0/06/Large-scale_AI_training_compute_%28FLOP%29_vs_Publication_date_%282017-2024%29.svg/220px-Large-scale_AI_training_compute_%28FLOP%29_vs_Publication_date_%282017-2024%29.svg.png", "//upload.wikimedia.org/wikipedia/commons/thumb/8/8f/The-Transformer-model-architecture.png/290px-The-Transformer-model-architecture.png"], "table": []},
{"title": "Dataset preprocessing", "text_content": "As machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indices are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding (BPE) and WordPiece. There are also special tokens serving as control characters, such as [MASK] for masked-out token (as used in BERT), and [UNK] (\"unknown\") for characters not appearing in the vocabulary. Also, some special symbols are used to denote special text formatting. For example, \"Ġ\" denotes a preceding whitespace in RoBERTa and GPT. \"##\" denotes continuation of a preceding word in BERT.[24] For example, the BPE tokenizer used by GPT-3 (Legacy) would split tokenizer: texts -> series of numerical \"tokens\" as Tokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \"padded\" until they match the length of the longest one. How many tokens are, on average, needed per word depends on the language of the dataset.[25][26] As an example, consider a tokenizer based on byte-pair encoding. In the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram, until a vocabulary of prescribed size is obtained (in case of GPT-3, the size is 50257).[27] After a tokenizer is trained, any text can be tokenized by it, as long as it does not contain characters not appearing in the initial-set of uni-grams.[28] A token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. An average word in another language encoded by such an English-optimized tokenizer is however split into suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages, for example for the Shan language from Myanmar. Even more widespread languages such as Portuguese and German have \"a premium of 50%\" compared to English.[29] Greedy tokenization also causes subtle problems with text completion.[30] In the context of training LLMs, datasets are typically cleaned by removing low-quality, duplicated, or toxic data.[31] Cleaned datasets can increase training efficiency and lead to improved downstream performance.[32][33] A trained LLM can be used to clean datasets for training a further LLM.[34] With the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it).[35] Training of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality. In these cases, synthetic data might be used. Microsoft's Phi series of LLMs is trained on textbook-like data generated by another LLM.[36]", "images": [], "table": []},
{"title": "Training and architecture", "text_content": "Reinforcement learning from human feedback (RLHF) through algorithms, such as proximal policy optimization, is used to further fine-tune a model based on a dataset of human preferences.[37] Using \"self-instruct\" approaches, LLMs have been able to bootstrap correct responses, replacing any naive responses, starting from human-generated corrections of a few cases. For example, in the instruction \"Write an essay about the main themes represented in Hamlet,\" an initial naive completion might be \"If you submit the essay after March 17, your grade will be reduced by 10% for each day of delay,\" based on the frequency of this textual sequence in the corpus.[38] The largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.[39][40][41] Most results previously achievable only by (costly) fine-tuning, can be achieved through prompt engineering, although limited to the scope of a single conversation (more precisely, limited to the scope of a context window).[42] In order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates \"soft\" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own \"relevance\" for calculating its own soft weights. For example, the small (i.e. 117M parameter sized) GPT-2 model has had twelve attention heads and a context window of only 1k tokens.[44] In its medium version it has 345M parameters and contains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.[28] The largest models, such as Google's Gemini 1.5, presented in February 2024, can have a context window sized up to 1 million (context window of 10 million was also \"successfully tested\").[45] Other models with large context windows includes Anthropic's Claude 2.1, with a context window of up to 200k tokens.[46] Note that this maximum refers to the number of input tokens and that the maximum number of output tokens differs from the input and is often smaller. For example, the GPT-4 Turbo model has a maximum output of 4096 tokens.[47] Length of a conversation that the model can take into account when generating its next answer is limited by the size of a context window, as well. If the length of a conversation, for example with ChatGPT, is longer than its context window, only the parts inside the context window are taken into account when generating the next answer, or the model needs to apply some algorithm to summarize the too distant parts of conversation. The shortcomings of making a context window larger include higher computational cost and possibly diluting the focus on local context, while making it smaller can cause a model to miss an important long-range dependency. Balancing them are a matter of experimentation and domain-specific considerations. A model may be pre-trained either to predict how the segment continues, or what is missing in the segment, given a segment from its training dataset.[48] It can be either Models may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus.[49] During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation. Substantial infrastructure is necessary for training the largest models.[50][51][52]", "images": ["//upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Multiple_attention_heads.png/290px-Multiple_attention_heads.png"], "table": []},
{"title": "Training cost", "text_content": "The qualifier \"large\" in \"large language model\" is inherently vague, as there is no definitive threshold for the number of parameters required to qualify as \"large\". As time goes on, what was previously considered \"large\" may evolve. GPT-1 of 2018 is usually considered the first LLM, even though it has only 0.117 billion parameters. The tendency towards larger models is visible in the list of large language models. Advances in software and hardware have reduced the cost substantially since 2020, such that in 2023 training of a 12-billion-parameter LLM computational cost is 72,300 A100-GPU-hours, while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80,000 and $1,600,000.[53][54][55] Since 2020, large sums were invested in increasingly large models. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million.[56] For Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.[57]", "images": ["//upload.wikimedia.org/wikipedia/commons/thumb/6/64/Estimated_training_cost_of_some_AI_models_-_2024_AI_index.jpg/330px-Estimated_training_cost_of_some_AI_models_-_2024_AI_index.jpg"], "table": []},
{"title": "Tool use", "text_content": "There are certain tasks that, in principle, cannot be solved by any LLM, at least not without the use of external tools or additional software. An example of such a task is responding to the user's input '354 * 139 = ', provided that the LLM has not already encountered a continuation of this calculation in its training corpus.[dubious – discuss] In such cases, the LLM needs to resort to running program code that calculates the result, which can then be included in its response.[dubious – discuss]: Another example is \"What is the time now? It is \", where a separate program interpreter would need to execute a code to get system time on the computer, so that the LLM can include it in its reply.[58][59] This basic strategy can be sophisticated with multiple attempts of generated programs, and other sampling strategies.[60] Generally, in order to get an LLM to use tools, one must fine-tune it for tool-use. If the number of tools is finite, then fine-tuning may be done just once. If the number of tools can grow arbitrarily, as with online API services, then the LLM can be fine-tuned to be able to read API documentation and call API correctly.[61][62] A simpler form of tool use is retrieval-augmented generation: the augmentation of an LLM with document retrieval. Given a query, a document retriever is called to retrieve the most relevant documents. This is usually done by encoding the query and the documents into vectors, then finding the documents with vectors (usually stored in a vector database) most similar to the vector of the query. The LLM then generates an output based on both the query and context included from the retrieved documents.[63]", "images": [], "table": []},
{"title": "Agency", "text_content": "An LLM is typically not an autonomous agent by itself, as it lacks the ability to interact with dynamic environments, recall past behaviors, and plan future actions, but can be transformed into one by integrating modules like profiling, memory, planning, and action.[64] The ReAct pattern, a portmanteau of \"Reason + Act\", constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an action, which is then executed in the environment.[65] The linguistic description of the environment given to the LLM planner can even be the LaTeX code of a paper describing the environment.[66] In the DEPS (\"Describe, Explain, Plan and Select\") method, an LLM is first connected to the visual world via image descriptions, then it is prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and environmental feedback it receives.[67] The Reflexion method[68] constructs an agent that learns over multiple episodes. At the end of each episode, the LLM is given the record of the episode, and prompted to think up \"lessons learned\", which would help it perform better at a subsequent episode. These \"lessons learned\" are given to the agent in the subsequent episodes.[citation needed] Monte Carlo tree search can use an LLM as rollout heuristic. When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.[69] For open-ended exploration, an LLM can be used to score observations for their \"interestingness\", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent.[70] Alternatively, it can propose increasingly difficult tasks for curriculum learning.[71] Instead of outputting individual actions, an LLM planner can also construct \"skills\", or functions for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.[71] LLM-powered agents can keep a long-term memory of its previous contexts, and the memory can be retrieved in the same way as Retrieval Augmented Generation. Multiple such agents can interact socially.[72]", "images": [], "table": []},
{"title": "Compression", "text_content": "Typically, LLMs are trained with single- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. The largest models typically have 100 billion parameters, requiring 200 gigabytes to load, which places them outside the range of most consumer electronics.[73] Post-training quantization[74] aims to decrease the space requirement by lowering precision of the parameters of a trained model, while preserving most of its performance.[75][76] The simplest form of quantization simply truncates all numbers to a given number of bits. It can be improved by using a different quantization codebook per layer. Further improvement can be done by applying different precisions to different parameters, with higher precision for particularly important parameters (\"outlier weights\").[77] See [78] for a visual guide. While quantized models are typically frozen, and only pre-quantized models are fine-tuned, quantized models can still be fine-tuned.[79]", "images": [], "table": []},
{"title": "Multimodality", "text_content": "Multimodality means \"having several modalities\", and a \"modality\" refers to a type of input or output, such as video, image, audio, text, proprioception, etc.[80] There have been many AI models trained specifically to ingest one modality and output another modality, such as AlexNet for image to label,[81] visual question answering for image-text to text,[82] and speech recognition for speech to text. A common method to create multimodal models out of an LLM is to \"tokenize\" the output of a trained encoder. Concretely, one can construct an LLM that can understand images as follows: take a trained LLM, and take a trained image encoder E {\\displaystyle E} . Make a small multilayered perceptron f {\\displaystyle f} , so that for any image y {\\displaystyle y} , the post-processed vector f ( E ( y ) ) {\\displaystyle f(E(y))} has the same dimensions as an encoded token. That is an \"image token\". Then, one can interleave text tokens and image tokens. The compound model is then fine-tuned on an image-text dataset. This basic construction can be applied with more sophistication to improve the model. The image encoder may be frozen to improve stability.[83] Flamingo demonstrated the effectiveness of the tokenization method, finetuning a pair of pretrained language model and image encoder to perform better on visual question answering than models trained from scratch.[84] Google PaLM model was fine-tuned into a multimodal model PaLM-E using the tokenization method, and applied to robotic control.[85] LLaMA models have also been turned multimodal using the tokenization method, to allow image inputs,[86] and video inputs.[87] GPT-4 can use both text and image as inputs[88] (although the vision component was not released to the public until GPT-4V[89]); Google DeepMind's Gemini is also multimodal.[90] Mistral introduced its own multimodel Pixtral 12B model in September 2024.[91]", "images": [], "table": []},
{"title": "Properties", "text_content": "The performance of an LLM after pretraining largely depends on the: \"Scaling laws\" are empirical statistical laws that predict LLM performance based on such factors. One particular scaling law (\"Chinchilla scaling\") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that:[92] { C = C 0 N D L = A N α + B D β + L 0 {\\displaystyle {\\begin{cases}C=C_{0}ND\\\\[6pt]L={\\frac {A}{N^{\\alpha }}}+{\\frac {B}{D^{\\beta }}}+L_{0}\\end{cases}}} where the variables are and the statistical hyper-parameters are Performance of bigger models on various tasks, when plotted on a log-log scale, appears as a linear extrapolation of performance achieved by smaller models. However, this linearity may be punctuated by \"break(s)\"[93] in the scaling law, where the slope of the line changes abruptly, and where larger models acquire \"emergent abilities\".[42][94] They arise from the complex interaction of the model's components and are not explicitly programmed or designed.[95] Furthermore, recent research has demonstrated that AI systems, including large language models, can employ heuristic reasoning akin to human cognition. They balance between exhaustive logical processing and the use of cognitive shortcuts (heuristics), adapting their reasoning strategies to optimize between accuracy and effort. This behavior aligns with principles of resource-rational human cognition, as discussed in classical theories of bounded rationality and dual-process theory.[96] The most intriguing among emergent abilities is in-context learning from example demonstrations.[97] In-context learning is involved in tasks, such as: Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.[103] Let x {\\displaystyle x} be the number of parameter count, and y {\\displaystyle y} be the performance of the model.", "images": ["//upload.wikimedia.org/wikipedia/commons/thumb/5/57/LLM_emergent_benchmarks.png/220px-LLM_emergent_benchmarks.png"], "table": []},
{"title": "Interpretation", "text_content": "Large language models by themselves are black boxes, and it is not clear how they can perform linguistic tasks. There are several methods for understanding how LLM work. Mechanistic interpretability aims to reverse-engineer LLM by discovering symbolic algorithms that approximate the inference performed by LLM. One example is Othello-GPT, where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of Othello board, and modifying the representation changes the predicted legal Othello moves in the correct way.[104][105] In another example, a small Transformer is trained on Karel programs. Similar to the Othello-GPT example, there is a linear representation of Karel program semantics, and modifying the representation changes output in the correct way. The model also generates correct programs that are on average shorter than those in the training set.[106] In another example, the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered, and it turned out they used discrete Fourier transform.[107] NLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs \"could (ever) understand natural language in some nontrivial sense\".[108] Proponents of \"LLM understanding\" believe that some LLM abilities, such as mathematical reasoning, imply an ability to \"understand\" certain concepts. A Microsoft team argued in 2023 that GPT-4 \"can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more\" and that GPT-4 \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system\": \"Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?\"[109][110] Ilya Sutskever argues that predicting the next word sometimes involves reasoning and deep insights, for example if the LLM has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation.[111] Some researchers characterize LLMs as \"alien intelligence\".[112][113] For example, Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien \"Shoggoths\", and believes that RLHF tuning creates a \"smiling facade\" obscuring the inner workings of the LLM: \"If you don't push it too far, the smiley face stays on. But then you give it [an unexpected] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human understanding.\"[114][115] In contrast, some proponents of the \"LLMs lack understanding\" school believe that existing LLMs are \"simply remixing and recombining existing writing\",[113] a phenomenon known as stochastic parrot, or they point to the deficits existing LLMs continue to have in prediction skills, reasoning skills, agency, and explainability.[108] For example, GPT-4 has natural deficits in planning and in real-time learning.[110] Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed \"hallucination\".[116] Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input.[117] Neuroscientist Terrence Sejnowski has argued that \"The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate\".[108] The matter of LLM's exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human like language.[108] These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL)[118] as a computational basis for using language as a model of learning tasks and understanding. The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct, British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language.[119][120]", "images": [], "table": []},
{"title": "Evaluation", "text_content": "The canonical measure of the performance of an LLM is its perplexity on a given text corpus. Perplexity measures how well a model predicts the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. In mathematical terms, perplexity is the exponential of the average negative log likelihood per token. log ⁡ ( Perplexity ) = − 1 N ∑ i = 1 N log ⁡ ( Pr ( token i ∣ context for token i ) ) {\\displaystyle \\log({\\text{Perplexity}})=-{\\frac {1}{N}}\\sum _{i=1}^{N}\\log(\\Pr({\\text{token}}_{i}\\mid {\\text{context for token}}_{i}))} Here, N {\\displaystyle N} is the number of tokens in the text corpus, and \"context for token i {\\displaystyle i} \" depends on the specific type of LLM. If the LLM is autoregressive, then \"context for token i {\\displaystyle i} \" is the segment of text appearing before token i {\\displaystyle i} . If the LLM is masked, then \"context for token i {\\displaystyle i} \" is the segment of text surrounding token i {\\displaystyle i} . Because language models may overfit to training data, models are usually evaluated by their perplexity on a test set.[49] This evaluation is potentially problematic for larger models which, as they are trained on increasingly large corpora of text, are increasingly likely to inadvertently include portions of any given test set.[1] In information theory, the concept of entropy is intricately linked to perplexity, a relationship notably established by Claude Shannon.[121] This relationship is mathematically expressed as Entropy = log 2 ⁡ ( Perplexity ) {\\displaystyle {\\text{Entropy}}=\\log _{2}({\\text{Perplexity}})} . Entropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges on whether the language model utilizes word-based or character-based tokenization. Notably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT) emerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different Large Language Models (LLMs), BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW, one can multiply it by the average number of tokens per word. In the evaluation and comparison of language models, cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model's enhanced capability for compression. This, in turn, reflects the model's proficiency in making accurate predictions. A large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge, commonsense reasoning, and mathematical problem-solving. One broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers, for example, (\"Have the San Jose Sharks won the Stanley Cup?\", \"No\").[122] A question answering task is considered \"open book\" if the model's prompt includes text from which the expected answer can be derived (for example, the previous question could be adjoined with some text which includes the sentence \"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\"[122]). Otherwise, the task is considered \"closed book\", and the model must draw on knowledge retained during training.[123] Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.[123] Evaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \"Alice was friends with Bob. Alice went to visit her friend, ____\".[1] Some composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, and HELM.[121][123] OpenAI has released tools for running composite benchmarks, but noted that the eval results are sensitive to the prompting method.[124][125] Some public datasets contain questions that are mislabeled, ambiguous, unanswerable, or otherwise of low-quality, which can be cleaned to give more reliable benchmark scores.[126] It was previously standard to report results on a heldout portion of an evaluation dataset after doing supervised fine-tuning on the remainder. It is now more common to evaluate a pre-trained model directly through prompting techniques, though researchers vary in the details of how they formulate prompts for particular tasks, particularly with respect to how many examples of solved tasks are adjoined to the prompt (i.e. the value of n in n-shot prompting). Because of the rapid pace of improvement of large language models, evaluation benchmarks have suffered from short lifespans, with state of the art models quickly \"saturating\" existing benchmarks, exceeding the performance of human annotators, leading to efforts to replace or augment the benchmark with more challenging tasks.[127] In addition, there are cases of \"shortcut learning\" wherein AIs sometimes \"cheat\" on multiple-choice tests by using statistical correlations in superficial test question wording in order to guess the correct responses, without necessarily understanding the actual question being asked.[108] Some datasets have been constructed adversarially, focusing on particular problems on which extant language models seem to have unusually poor performance compared to humans. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods to which they were repeatedly exposed during training. For example, an LLM may answer \"No\" to the question \"Can you teach an old dog new tricks?\" because of its exposure to the English idiom you can't teach an old dog new tricks, even though this is not literally true.[128] Another example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time the datasets were created state of the art language models had poor accuracy on them. For example: BERT selects b) as the most likely completion, though the correct answer is d).[129]", "images": [], "table": []},
{"title": "Wider impact", "text_content": "In 2023, Nature Biomedical Engineering wrote that \"it is no longer possible to accurately distinguish\" human-written text from text created by large language models, and that \"It is all but certain that general-purpose large language models will rapidly proliferate... It is a rather safe bet that they will change many industries over time.\"[130] Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years, and could expose to automation 300 million jobs globally.[131][132] Memorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates[133] or up to about 7%.[134] A 2023 study showed that when ChatGPT 3.5 turbo was prompted to repeat the same word indefinitely, after a few hundreds of repetitions, it would start outputting excerpts from its training data.[135] Some commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse.[136] For example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.[137] The potential presence of \"sleeper agents\" within LLM models is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions.[138] LLM applications accessible to the public, like ChatGPT or Claude, typically incorporate safety measures designed to filter out harmful content. However, implementing these controls effectively has proven challenging. For instance, a 2023 study[139] proposed a method for circumventing LLM safety systems. Similarly, Yongge Wang[140] illustrated in 2024 how a potential criminal could potentially bypass ChatGPT 4o's safety controls to obtain information on establishing a drug trafficking operation. While LLMs have shown remarkable capabilities in generating human-like text, they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics, such as those based on race, gender, language, and cultural groups.[141] Since English data is overrepresented in current large language models' training data, it may also downplay non-English views.[142] AI models can reinforce a wide range of stereotypes, including those based on gender, ethnicity, age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.[143] Notably, gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms.[141] For example, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men.[144] Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.[145]", "images": [], "table": []},
{"title": "See also", "text_content": "", "images": [], "table": []},
{"title": "References", "text_content": "", "images": [], "table": []},
{"title": "Further reading", "text_content": "", "images": [], "table": []}
]