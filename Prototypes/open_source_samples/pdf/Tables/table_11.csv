,from the website newser.com (mean length: 215 words). We then generated completions of these titles and subtitles
0,from four language models ranging in size from 125M to 175B (GPT-3) parameters (mean length: 200 words). For each
1,"model, we presented around 80 US-based participants with a quiz consisting of these real titles and subtitles followed"
2,by either the human written article or the article generated by the model4. Participants were asked to select whether the
3,"article was very likely written by a human, more likely written by a human, I dont know, more likely written by"
4,"a machine, or very likely written by a machine."
5,The articles we selected were not in the models training data and the model outputs were formatted and selected
6,programmatically to prevent human cherry-picking. All models used the same context to condition outputs on and were
