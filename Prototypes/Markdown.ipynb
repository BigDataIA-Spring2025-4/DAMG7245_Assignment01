{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6b9d3f2-3034-4e38-ab31-ccd315b4c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adfce3a0-9ad0-4cff-b715-c944ae7b3793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Configuration\n",
    "# AWS_ACCESS_KEY = \"AKIASBGQLOUGU4LBU7PD\"\n",
    "# AWS_SECRET_KEY = \"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\"\n",
    "# S3_BUCKET_NAME = \"scrapedimages\"\n",
    "# S3_REGION = \"us-east-2\"\n",
    "\n",
    "file_path = \"C://Users//ankit//OneDrive//Desktop//web-scraping//images\"  \n",
    "bucket_name = \"scrapedimages\" \n",
    "s3_key = \"IdHZLflg/zy/9MRHCLYp1arWRZMSnLj7zyuZzK7K\"\n",
    "aws_access_key = \"KIASBGQLOUGU4LBU7PD\"  \n",
    "aws_secret_key = \"IdHZLflg/zy/9MRHCLYp1arWRZMSnLj7zyuZzK7K\" \n",
    "region = \"us-east-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b726cf48-3f97-40cb-9a39-1b94b5d62d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(file_path, bucket_name, s3_key, aws_access_key, aws_secret_key, region):\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key,\n",
    "        aws_secret_access_key=aws_secret_key,\n",
    "        region_name=region\n",
    "    )\n",
    "    try:\n",
    "        s3.upload_file(file_path, bucket_name, s3_key)  # No ACL specified\n",
    "        s3_url = f\"https://{bucket_name}.s3.{region}.amazonaws.com/{s3_key}\"\n",
    "        print(f\"Uploaded {file_path} to {s3_url}\")\n",
    "        return s3_url\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload {file_path} to S3: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d270b012-24c5-4bb2-a881-d843ba95f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape images\n",
    "def scrape_images(url, save_directory=\"images\"):\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    img_tags = soup.find_all('img')\n",
    "    image_urls = []\n",
    "    for img_tag in img_tags:\n",
    "        img_url = img_tag.get('src')\n",
    "        if img_url:\n",
    "            img_url = urljoin(url, img_url)\n",
    "            img_name = os.path.basename(img_url)\n",
    "            img_path = os.path.join(save_directory, img_name)\n",
    "            try:\n",
    "                img_data = requests.get(img_url).content\n",
    "                with open(img_path, 'wb') as img_file:\n",
    "                    img_file.write(img_data)\n",
    "                s3_key = f\"scraped_images/{img_name}\"\n",
    "                s3_url = upload_to_s3(img_path, S3_BUCKET_NAME, s3_key, AWS_ACCESS_KEY, AWS_SECRET_KEY, S3_REGION)\n",
    "                if s3_url:\n",
    "                    image_urls.append(s3_url)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {img_url}: {e}\")\n",
    "    return image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a389e49d-4ace-4579-95a2-7e67755cf2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tables(url, output_directory=\"tables\"):\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    table_files = []\n",
    "    for i, table in enumerate(tables):\n",
    "        try:\n",
    "            # Wrap the table HTML string in StringIO before passing to pd.read_html\n",
    "            df = pd.read_html(StringIO(str(table)))[0]\n",
    "            file_path = os.path.join(output_directory, f\"table_{i + 1}.csv\")\n",
    "            df.to_csv(file_path, index=False)\n",
    "            table_files.append(file_path)\n",
    "            print(f\"Saved table {i + 1} to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse table {i + 1}: {e}\")\n",
    "    return table_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6cf7eb7-881e-4f0e-8dcf-46fb75ab6ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape metadata\n",
    "def scrape_metadata(url):\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    metadata = {\n",
    "        \"title\": soup.title.string if soup.title else \"No Title\",\n",
    "        \"meta_tags\": [\n",
    "            {\"name\": tag.get(\"name\"), \"content\": tag.get(\"content\")}\n",
    "            for tag in soup.find_all(\"meta\")\n",
    "        ]\n",
    "    }\n",
    "    metadata_file = \"metadata.json\"\n",
    "    with open(metadata_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(metadata, file, indent=4)\n",
    "    return metadata_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "053625e0-4e93-49a8-9f72-113421308a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display results in Markdown format\n",
    "def create_markdown(metadata_file, table_files, image_urls):\n",
    "    # Display Metadata\n",
    "    print(\"# Metadata\")\n",
    "    with open(metadata_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        metadata = json.load(file)\n",
    "        display(Markdown(f\"```json\\n{json.dumps(metadata, indent=4)}\\n```\"))\n",
    "\n",
    "    # Display Images\n",
    "    print(\"# Images\")\n",
    "    for img_url in image_urls:\n",
    "        display(Markdown(f\"![Image]({img_url})\"))\n",
    "\n",
    "    # Display Tables\n",
    "    print(\"# Tables\")\n",
    "    for table_file in table_files:\n",
    "        df = pd.read_csv(table_file)\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8267e11-a0ee-4ab8-bdd3-91ff818941d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping images...\n",
      "Uploaded images\\gfg-gg-logo.svg to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/gfg-gg-logo.svg\n",
      "Uploaded images\\Google-news.svg to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/Google-news.svg\n",
      "Uploaded images\\Python-Data-Types.webp to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/Python-Data-Types.webp\n",
      "Uploaded images\\python-tutorial-2.webp to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/python-tutorial-2.webp\n",
      "Uploaded images\\sb7ciorr5k5t22woqkes to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/sb7ciorr5k5t22woqkes\n",
      "Uploaded images\\Google-news.svg to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/Google-news.svg\n",
      "Uploaded images\\premium_oin_rbar_min.png to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/premium_oin_rbar_min.png\n",
      "Uploaded images\\gfgFooterLogo.png to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/gfgFooterLogo.png\n",
      "Uploaded images\\googleplay.png to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/googleplay.png\n",
      "Uploaded images\\appstore.png to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/appstore.png\n",
      "Uploaded images\\suggestChangeIcon.png to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/suggestChangeIcon.png\n",
      "Uploaded images\\createImprovementIcon.png to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/createImprovementIcon.png\n",
      "Scraping tables...\n",
      "Saved table 1 to tables\\table_1.csv\n",
      "Saved table 2 to tables\\table_2.csv\n",
      "Saved table 3 to tables\\table_3.csv\n",
      "Scraping metadata...\n",
      "Displaying results in Markdown...\n",
      "# Metadata\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "    \"title\": \"Python Tutorial | Learn Python Programming Language\",\n",
       "    \"meta_tags\": [\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": null\n",
       "        },\n",
       "        {\n",
       "            \"name\": \"viewport\",\n",
       "            \"content\": \"width=device-width, initial-scale=1.0, minimum-scale=0.5, maximum-scale=3.0\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": \"theme-color\",\n",
       "            \"content\": \"#308D46\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": \"robots\",\n",
       "            \"content\": \"index, follow, max-image-preview:large, max-snippet:-1\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": \"image\",\n",
       "            \"content\": \"https://media.geeksforgeeks.org/wp-content/cdn-uploads/gfg_200x200-min.png\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"image/png\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"200\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"200\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": \"facebook-domain-verification\",\n",
       "            \"content\": \"xo7t4ve2wn3ywfkjdvwbrk01pvdond\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"Python Tutorial | Learn Python Programming Language - GeeksforGeeks\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": \"description\",\n",
       "            \"content\": \"Python is a versatile and beginner-friendly programming language widely used in web development, data science, and AI, known for its simplicity, extensive libraries, and strong community support.\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"https://www.geeksforgeeks.org/python-programming-language-tutorial/\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": \"keywords\",\n",
       "            \"content\": \"Python Tutorial, Python Programming, Data Science, Machine Learning, Object-Oriented Programming, Python Libraries, Web Development Frameworks, Python Exception Handling, File Handling in Python, Python Data Types, Python Functions, Python Collections Module, Python Database Handling, Python Career Opportunities, Python Features and Advantages\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"GeeksforGeeks\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"https://media.geeksforgeeks.org/wp-content/uploads/20241129125555314992/Python-Data-Types.webp\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"Python\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"Tutorials\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"python\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"article\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"en_US\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"2024-03-01 13:15:55+00:00\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"2025-01-15 19:55:52+00:00\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"2025-01-15 19:55:52+00:00\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"https://media.geeksforgeeks.org/wp-content/uploads/20241129125555314992/Python-Data-Types.webp\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": null,\n",
       "            \"content\": \"Python is a versatile and beginner-friendly programming language widely used in web development, data science, and AI, known for its simplicity, extensive libraries, and strong community support.\"\n",
       "        },\n",
       "        {\n",
       "            \"name\": \"msapplication-TileImage\",\n",
       "            \"content\": \"https://www.geeksforgeeks.org/wp-content/uploads/gfg_200X200.png\"\n",
       "        }\n",
       "    ]\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Images\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "![Image](https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/gfg-gg-logo.svg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![Image](https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/Google-news.svg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![Image](https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/Python-Data-Types.webp)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![Image](https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/python-tutorial-2.webp)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![Image](https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/sb7ciorr5k5t22woqkes)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![Image](https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/Google-news.svg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![Image](https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/premium_oin_rbar_min.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![Image](https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/gfgFooterLogo.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![Image](https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/googleplay.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![Image](https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/appstore.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![Image](https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/suggestChangeIcon.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![Image](https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/createImprovementIcon.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tables\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Python</th>\n",
       "      <th>C</th>\n",
       "      <th>C++</th>\n",
       "      <th>Java</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Type</td>\n",
       "      <td>Interpreted</td>\n",
       "      <td>Compiled</td>\n",
       "      <td>Compiled</td>\n",
       "      <td>Compiled and Interpreted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paradigm</td>\n",
       "      <td>Multi-paradigm (object-oriented, procedural, f...</td>\n",
       "      <td>Procedural, structured</td>\n",
       "      <td>Multi-paradigm (procedural, object-oriented, g...</td>\n",
       "      <td>Object-oriented, structured</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Memory Management</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Automatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Syntax</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Complex</td>\n",
       "      <td>Complex</td>\n",
       "      <td>Complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Use Cases</td>\n",
       "      <td>Web development, data analysis, machine learning</td>\n",
       "      <td>System programming, embedded systems, game dev...</td>\n",
       "      <td>System programming, game development, high-per...</td>\n",
       "      <td>Large-scale applications, enterprise software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Notable Frameworks/Libraries</td>\n",
       "      <td>Django, Flask</td>\n",
       "      <td>Standard Library</td>\n",
       "      <td>Standard Library, Boost</td>\n",
       "      <td>Spring, Hibernate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Community Support</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Job Market</td>\n",
       "      <td>Abundant</td>\n",
       "      <td>Abundant</td>\n",
       "      <td>Abundant</td>\n",
       "      <td>Abundant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Feature  \\\n",
       "0                          Type   \n",
       "1                      Paradigm   \n",
       "2             Memory Management   \n",
       "3                        Syntax   \n",
       "4                     Use Cases   \n",
       "5  Notable Frameworks/Libraries   \n",
       "6             Community Support   \n",
       "7                    Job Market   \n",
       "\n",
       "                                              Python  \\\n",
       "0                                        Interpreted   \n",
       "1  Multi-paradigm (object-oriented, procedural, f...   \n",
       "2                                          Automatic   \n",
       "3                                             Simple   \n",
       "4   Web development, data analysis, machine learning   \n",
       "5                                      Django, Flask   \n",
       "6                                             Strong   \n",
       "7                                           Abundant   \n",
       "\n",
       "                                                   C  \\\n",
       "0                                           Compiled   \n",
       "1                             Procedural, structured   \n",
       "2                                             Manual   \n",
       "3                                            Complex   \n",
       "4  System programming, embedded systems, game dev...   \n",
       "5                                   Standard Library   \n",
       "6                                             Strong   \n",
       "7                                           Abundant   \n",
       "\n",
       "                                                 C++  \\\n",
       "0                                           Compiled   \n",
       "1  Multi-paradigm (procedural, object-oriented, g...   \n",
       "2                                             Manual   \n",
       "3                                            Complex   \n",
       "4  System programming, game development, high-per...   \n",
       "5                            Standard Library, Boost   \n",
       "6                                             Strong   \n",
       "7                                           Abundant   \n",
       "\n",
       "                                            Java  \n",
       "0                       Compiled and Interpreted  \n",
       "1                    Object-oriented, structured  \n",
       "2                                      Automatic  \n",
       "3                                        Complex  \n",
       "4  Large-scale applications, enterprise software  \n",
       "5                              Spring, Hibernate  \n",
       "6                                         Strong  \n",
       "7                                       Abundant  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google</td>\n",
       "      <td>Uses Python for various applications, includin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Instagram</td>\n",
       "      <td>The backend of Instagram is built using Python...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spotify</td>\n",
       "      <td>Python is used for data analysis and backend s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dropbox</td>\n",
       "      <td>Python powers the desktop client of Dropbox, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netflix</td>\n",
       "      <td>Python helps Netflix with data analysis and ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>One of the largest online communities, Reddit,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Uber</td>\n",
       "      <td>Uber uses Python for various features, includi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pinterest</td>\n",
       "      <td>Python plays a key role in the backend of Pint...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Company                                        Description\n",
       "0     Google  Uses Python for various applications, includin...\n",
       "1  Instagram  The backend of Instagram is built using Python...\n",
       "2    Spotify  Python is used for data analysis and backend s...\n",
       "3    Dropbox  Python powers the desktop client of Dropbox, m...\n",
       "4    Netflix  Python helps Netflix with data analysis and ma...\n",
       "5     Reddit  One of the largest online communities, Reddit,...\n",
       "6       Uber  Uber uses Python for various features, includi...\n",
       "7  Pinterest  Python plays a key role in the backend of Pint..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Career</th>\n",
       "      <th>Average Salary (INR) Per Annum</th>\n",
       "      <th>Average Salary (USD) Per Annum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Python Developer</td>\n",
       "      <td>₹500,000 – ₹1,200,000</td>\n",
       "      <td>$60,000 – $110,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>₹600,000 – ₹1,500,000</td>\n",
       "      <td>$70,000 – $130,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>₹700,000 – ₹1,800,000</td>\n",
       "      <td>$75,000 – $140,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Full Stack Developer</td>\n",
       "      <td>₹600,000 – ₹1,300,000</td>\n",
       "      <td>$65,000 – $120,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DevOps Engineer</td>\n",
       "      <td>₹800,000 – ₹2,000,000</td>\n",
       "      <td>$80,000 – $140,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Automation Engineer</td>\n",
       "      <td>₹500,000 – ₹1,200,000</td>\n",
       "      <td>$55,000 – $100,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>₹400,000 – ₹900,000</td>\n",
       "      <td>$50,000 – $90,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>₹500,000 – ₹1,500,000</td>\n",
       "      <td>$65,000 – $120,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Backend Developer</td>\n",
       "      <td>₹600,000 – ₹1,300,000</td>\n",
       "      <td>$70,000 – $125,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AI Engineer</td>\n",
       "      <td>₹900,000 – ₹2,500,000</td>\n",
       "      <td>$90,000 – $160,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Career Average Salary (INR) Per Annum  \\\n",
       "0           Python Developer          ₹500,000 – ₹1,200,000   \n",
       "1             Data Scientist          ₹600,000 – ₹1,500,000   \n",
       "2  Machine Learning Engineer          ₹700,000 – ₹1,800,000   \n",
       "3       Full Stack Developer          ₹600,000 – ₹1,300,000   \n",
       "4            DevOps Engineer          ₹800,000 – ₹2,000,000   \n",
       "5        Automation Engineer          ₹500,000 – ₹1,200,000   \n",
       "6               Data Analyst            ₹400,000 – ₹900,000   \n",
       "7          Software Engineer          ₹500,000 – ₹1,500,000   \n",
       "8          Backend Developer          ₹600,000 – ₹1,300,000   \n",
       "9                AI Engineer          ₹900,000 – ₹2,500,000   \n",
       "\n",
       "  Average Salary (USD) Per Annum  \n",
       "0             $60,000 – $110,000  \n",
       "1             $70,000 – $130,000  \n",
       "2             $75,000 – $140,000  \n",
       "3             $65,000 – $120,000  \n",
       "4             $80,000 – $140,000  \n",
       "5             $55,000 – $100,000  \n",
       "6              $50,000 – $90,000  \n",
       "7             $65,000 – $120,000  \n",
       "8             $70,000 – $125,000  \n",
       "9             $90,000 – $160,000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.geeksforgeeks.org/python-programming-language-tutorial/\"\n",
    "\n",
    "    # Scrape data\n",
    "    print(\"Scraping images...\")\n",
    "    image_urls = scrape_images(url)\n",
    "\n",
    "    print(\"Scraping tables...\")\n",
    "    table_files = scrape_tables(url)\n",
    "\n",
    "    print(\"Scraping metadata...\")\n",
    "    metadata_file = scrape_metadata(url)\n",
    "\n",
    "    # Generate and display Markdown\n",
    "    print(\"Displaying results in Markdown...\")\n",
    "    create_markdown(metadata_file, table_files, image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62a6351b-06c7-4a4d-a651-4e16fcba2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "import hashlib\n",
    "from IPython.display import display, Markdown\n",
    "import docling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc605a-084c-4b48-8a07-257333d2066c",
   "metadata": {},
   "source": [
    "## S3 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fe00cce-9b33-45ed-930b-8b066bbcb1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY = \"AKIASBGQLOUGU4LBU7PD\"\n",
    "AWS_SECRET_KEY = \"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\"\n",
    "S3_BUCKET_NAME = \"scrapedimages\"\n",
    "S3_REGION = \"us-east-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44b98ee-c6d4-405a-89a8-e5207a130585",
   "metadata": {},
   "source": [
    "## Function to generate a hash for image validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c59c8be2-c2e2-4d49-8eb0-74f9a8fc8708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hash(file_path):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de1ec6-6913-4e2d-aff0-9b3eb669f7d5",
   "metadata": {},
   "source": [
    "## Function to upload files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0d7a914-b21b-4b5b-963e-e5d8f01dc101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(file_path, bucket_name, s3_key, aws_access_key, aws_secret_key, region):\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key,\n",
    "        aws_secret_access_key=aws_secret_key,\n",
    "        region_name=region\n",
    "    )\n",
    "    try:\n",
    "        s3.upload_file(file_path, bucket_name, s3_key, ExtraArgs={'ACL': 'public-read'})\n",
    "        s3_url = f\"https://{bucket_name}.s3.{region}.amazonaws.com/{s3_key}\"\n",
    "        print(f\"Uploaded {file_path} to {s3_url}\")\n",
    "        return s3_url\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload {file_path} to S3: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de475cd3-ce26-48bb-aa2c-0e43cc8d045a",
   "metadata": {},
   "source": [
    "## Function to scrape images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4598823d-0b7c-4ad4-8f47-3d3c5e75184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_images(url, save_directory=\"images\"):\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    img_tags = soup.find_all('img')\n",
    "    image_urls = []\n",
    "    hashes = set()  # To store hashes of uploaded images\n",
    "    for img_tag in img_tags:\n",
    "        img_url = img_tag.get('src')\n",
    "        if img_url:\n",
    "            img_url = urljoin(url, img_url)\n",
    "            img_name = os.path.basename(img_url)\n",
    "            img_path = os.path.join(save_directory, img_name)\n",
    "            try:\n",
    "                img_data = requests.get(img_url).content\n",
    "                with open(img_path, 'wb') as img_file:\n",
    "                    img_file.write(img_data)\n",
    "\n",
    "                img_hash = generate_hash(img_path)\n",
    "                if img_hash in hashes:  # Skip duplicates\n",
    "                    print(f\"Duplicate image skipped: {img_url}\")\n",
    "                    continue\n",
    "\n",
    "                hashes.add(img_hash)  # Add hash to the set\n",
    "                s3_key = f\"scraped_images/{img_name}\"\n",
    "                s3_url = upload_to_s3(img_path, S3_BUCKET_NAME, s3_key, AWS_ACCESS_KEY, AWS_SECRET_KEY, S3_REGION)\n",
    "                if s3_url:\n",
    "                    image_urls.append(s3_url)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {img_url}: {e}\")\n",
    "    return image_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b860c-d2fa-4b34-9e5c-83215ed62f4e",
   "metadata": {},
   "source": [
    "## Function to scrape tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85ee0a03-8693-497c-abee-3193435572e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tables(url, output_directory=\"tables\"):\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    table_files = []\n",
    "    for i, table in enumerate(tables):\n",
    "        try:\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            file_path = os.path.join(output_directory, f\"table_{i + 1}.csv\")\n",
    "            df.to_csv(file_path, index=False)\n",
    "            table_files.append(file_path)\n",
    "            print(f\"Saved table {i + 1} to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse table {i + 1}: {e}\")\n",
    "    return table_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28176cfb-f47c-47c0-a2fd-ada604dd72a4",
   "metadata": {},
   "source": [
    "## Function to scrape metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d51db3d-d3ff-41e1-9e69-a3ccfc4768a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_metadata(url):\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    metadata = {\n",
    "        \"title\": soup.title.string if soup.title else \"No Title\",\n",
    "        \"meta_tags\": [\n",
    "            {\"name\": tag.get(\"name\"), \"content\": tag.get(\"content\")}\n",
    "            for tag in soup.find_all(\"meta\")\n",
    "        ]\n",
    "    }\n",
    "    metadata_file = \"metadata.json\"\n",
    "    with open(metadata_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(metadata, file, indent=4)\n",
    "    return metadata_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c51241-5875-465c-b86c-d2e8e1946239",
   "metadata": {},
   "source": [
    "## Function to save markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc05b352-637f-4af8-9473-ae85040b88ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_markdown(metadata_file, table_files, image_urls, output_file=\"output.md\"):\n",
    "    doc = docling.Document()\n",
    "\n",
    "    # Add Metadata\n",
    "    doc.add_heading(\"Metadata\", level=1)\n",
    "    with open(metadata_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        metadata = json.load(file)\n",
    "        doc.add_codeblock(json.dumps(metadata, indent=4), language=\"json\")\n",
    "\n",
    "    # Add Images\n",
    "    doc.add_heading(\"Images\", level=1)\n",
    "    for img_url in image_urls:\n",
    "        doc.add_paragraph(f\"![Image]({img_url})\")\n",
    "\n",
    "    # Add Tables\n",
    "    doc.add_heading(\"Tables\", level=1)\n",
    "    for i, table_file in enumerate(table_files):\n",
    "        df = pd.read_csv(table_file)\n",
    "        table_text = df.to_markdown(index=False)  # Convert DataFrame to Markdown format\n",
    "        doc.add_heading(f\"Table {i + 1}\", level=2)\n",
    "        doc.add_codeblock(table_text, language=\"markdown\")\n",
    "\n",
    "    # Save markdown file\n",
    "    doc.save(output_file)\n",
    "    print(f\"Markdown file saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af225b06-9d2e-45c3-97dd-4d3308dba535",
   "metadata": {},
   "source": [
    "## Main execution block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb9fb2b1-56d5-43bd-94f6-5f5f2b7303c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping images...\n",
      "Failed to upload images\\gfg-gg-logo.svg to S3: Failed to upload images\\gfg-gg-logo.svg to scrapedimages/scraped_images/gfg-gg-logo.svg: An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs\n",
      "Failed to upload images\\Google-news.svg to S3: Failed to upload images\\Google-news.svg to scrapedimages/scraped_images/Google-news.svg: An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs\n",
      "Failed to upload images\\Python-Data-Types.webp to S3: Failed to upload images\\Python-Data-Types.webp to scrapedimages/scraped_images/Python-Data-Types.webp: An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs\n",
      "Failed to upload images\\python-tutorial-2.webp to S3: Failed to upload images\\python-tutorial-2.webp to scrapedimages/scraped_images/python-tutorial-2.webp: An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs\n",
      "Failed to upload images\\sb7ciorr5k5t22woqkes to S3: Failed to upload images\\sb7ciorr5k5t22woqkes to scrapedimages/scraped_images/sb7ciorr5k5t22woqkes: An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs\n",
      "Duplicate image skipped: https://media.geeksforgeeks.org/auth-dashboard-uploads/Google-news.svg\n",
      "Failed to upload images\\premium_oin_rbar_min.png to S3: Failed to upload images\\premium_oin_rbar_min.png to scrapedimages/scraped_images/premium_oin_rbar_min.png: An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs\n",
      "Failed to upload images\\gfgFooterLogo.png to S3: Failed to upload images\\gfgFooterLogo.png to scrapedimages/scraped_images/gfgFooterLogo.png: An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs\n",
      "Failed to upload images\\googleplay.png to S3: Failed to upload images\\googleplay.png to scrapedimages/scraped_images/googleplay.png: An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs\n",
      "Failed to upload images\\appstore.png to S3: Failed to upload images\\appstore.png to scrapedimages/scraped_images/appstore.png: An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs\n",
      "Failed to upload images\\suggestChangeIcon.png to S3: Failed to upload images\\suggestChangeIcon.png to scrapedimages/scraped_images/suggestChangeIcon.png: An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs\n",
      "Failed to upload images\\createImprovementIcon.png to S3: Failed to upload images\\createImprovementIcon.png to scrapedimages/scraped_images/createImprovementIcon.png: An error occurred (AccessControlListNotSupported) when calling the PutObject operation: The bucket does not allow ACLs\n",
      "Scraping tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_13896\\3193292134.py:10: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_13896\\3193292134.py:10: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_13896\\3193292134.py:10: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved table 1 to tables\\table_1.csv\n",
      "Saved table 2 to tables\\table_2.csv\n",
      "Saved table 3 to tables\\table_3.csv\n",
      "Scraping metadata...\n",
      "Creating final markdown file...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'docling' has no attribute 'Document'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Generate and save Markdown file\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating final markdown file...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[43msave_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_urls\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m, in \u001b[0;36msave_markdown\u001b[1;34m(metadata_file, table_files, image_urls, output_file)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_markdown\u001b[39m(metadata_file, table_files, image_urls, output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.md\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdocling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDocument\u001b[49m()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Add Metadata\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     doc\u001b[38;5;241m.\u001b[39madd_heading(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'docling' has no attribute 'Document'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.geeksforgeeks.org/python-programming-language-tutorial/\"\n",
    "\n",
    "    # Scrape data\n",
    "    print(\"Scraping images...\")\n",
    "    image_urls = scrape_images(url)\n",
    "\n",
    "    print(\"Scraping tables...\")\n",
    "    table_files = scrape_tables(url)\n",
    "\n",
    "    print(\"Scraping metadata...\")\n",
    "    metadata_file = scrape_metadata(url)\n",
    "\n",
    "    # Generate and save Markdown file\n",
    "    print(\"Creating final markdown file...\")\n",
    "    save_markdown(metadata_file, table_files, image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb0585d7-a0c1-4177-af10-00508b0ec7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping images...\n",
      "Uploaded images\\gfg-gg-logo.svg to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/gfg-gg-logo.svg\n",
      "Uploaded images\\Google-news.svg to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/Google-news.svg\n",
      "Uploaded images\\Python-Data-Types.webp to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/Python-Data-Types.webp\n",
      "Uploaded images\\python-tutorial-2.webp to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/python-tutorial-2.webp\n",
      "Uploaded images\\sb7ciorr5k5t22woqkes to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/sb7ciorr5k5t22woqkes\n",
      "Duplicate image skipped: https://media.geeksforgeeks.org/auth-dashboard-uploads/Google-news.svg\n",
      "Uploaded images\\premium_oin_rbar_min.png to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/premium_oin_rbar_min.png\n",
      "Uploaded images\\gfgFooterLogo.png to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/gfgFooterLogo.png\n",
      "Uploaded images\\googleplay.png to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/googleplay.png\n",
      "Uploaded images\\appstore.png to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/appstore.png\n",
      "Uploaded images\\suggestChangeIcon.png to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/suggestChangeIcon.png\n",
      "Uploaded images\\createImprovementIcon.png to https://scrapedimages.s3.us-east-2.amazonaws.com/scraped_images/createImprovementIcon.png\n",
      "Scraping tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_13896\\2589647066.py:86: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_13896\\2589647066.py:86: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_13896\\2589647066.py:86: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved table 1 to tables\\table_1.csv\n",
      "Saved table 2 to tables\\table_2.csv\n",
      "Saved table 3 to tables\\table_3.csv\n",
      "Scraping metadata...\n",
      "Creating final markdown file...\n",
      "Markdown file saved: output.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "import hashlib\n",
    "\n",
    "# S3 Configuration\n",
    "AWS_ACCESS_KEY = \"AKIASBGQLOUGU4LBU7PD\"\n",
    "AWS_SECRET_KEY = \"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\"\n",
    "S3_BUCKET_NAME = \"scrapedimages\"\n",
    "S3_REGION = \"us-east-2\"\n",
    "\n",
    "# Function to generate a hash for image validation\n",
    "def generate_hash(file_path):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "# Function to upload files to S3\n",
    "def upload_to_s3(file_path, bucket_name, s3_key, aws_access_key, aws_secret_key, region):\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key,\n",
    "        aws_secret_access_key=aws_secret_key,\n",
    "        region_name=region\n",
    "    )\n",
    "    try:\n",
    "        s3.upload_file(file_path, bucket_name, s3_key)\n",
    "        s3_url = f\"https://{bucket_name}.s3.{region}.amazonaws.com/{s3_key}\"\n",
    "        print(f\"Uploaded {file_path} to {s3_url}\")\n",
    "        return s3_url\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload {file_path} to S3: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to scrape images\n",
    "def scrape_images(url, save_directory=\"images\"):\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    img_tags = soup.find_all('img')\n",
    "    image_urls = []\n",
    "    hashes = set()  # To store hashes of uploaded images\n",
    "    for img_tag in img_tags:\n",
    "        img_url = img_tag.get('src')\n",
    "        if img_url:\n",
    "            img_url = urljoin(url, img_url)\n",
    "            img_name = os.path.basename(img_url)\n",
    "            img_path = os.path.join(save_directory, img_name)\n",
    "            try:\n",
    "                img_data = requests.get(img_url).content\n",
    "                with open(img_path, 'wb') as img_file:\n",
    "                    img_file.write(img_data)\n",
    "\n",
    "                img_hash = generate_hash(img_path)\n",
    "                if img_hash in hashes:  # Skip duplicates\n",
    "                    print(f\"Duplicate image skipped: {img_url}\")\n",
    "                    continue\n",
    "\n",
    "                hashes.add(img_hash)  # Add hash to the set\n",
    "                s3_key = f\"scraped_images/{img_name}\"\n",
    "                s3_url = upload_to_s3(img_path, S3_BUCKET_NAME, s3_key, AWS_ACCESS_KEY, AWS_SECRET_KEY, S3_REGION)\n",
    "                if s3_url:\n",
    "                    image_urls.append(s3_url)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {img_url}: {e}\")\n",
    "    return image_urls\n",
    "\n",
    "# Function to scrape tables\n",
    "def scrape_tables(url, output_directory=\"tables\"):\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    table_files = []\n",
    "    for i, table in enumerate(tables):\n",
    "        try:\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            file_path = os.path.join(output_directory, f\"table_{i + 1}.csv\")\n",
    "            df.to_csv(file_path, index=False)\n",
    "            table_files.append(file_path)\n",
    "            print(f\"Saved table {i + 1} to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse table {i + 1}: {e}\")\n",
    "    return table_files\n",
    "\n",
    "# Function to scrape metadata\n",
    "def scrape_metadata(url):\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    metadata = {\n",
    "        \"title\": soup.title.string if soup.title else \"No Title\",\n",
    "        \"meta_tags\": [\n",
    "            {\"name\": tag.get(\"name\"), \"content\": tag.get(\"content\")}\n",
    "            for tag in soup.find_all(\"meta\")\n",
    "        ]\n",
    "    }\n",
    "    metadata_file = \"metadata.json\"\n",
    "    with open(metadata_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(metadata, file, indent=4)\n",
    "    return metadata_file\n",
    "\n",
    "# Function to save markdown file\n",
    "def save_markdown(metadata_file, table_files, image_urls, output_file=\"output.md\"):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as md:\n",
    "        # Add Metadata\n",
    "        md.write(\"# Metadata\\n\")\n",
    "        with open(metadata_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            metadata = json.load(file)\n",
    "            md.write(\"```\\n\")\n",
    "            json.dump(metadata, md, indent=4)\n",
    "            md.write(\"\\n```\\n\\n\")\n",
    "\n",
    "        # Add Images\n",
    "        md.write(\"# Images\\n\")\n",
    "        for img_url in image_urls:\n",
    "            md.write(f\"![Image]({img_url})\\n\\n\")\n",
    "\n",
    "        # Add Tables\n",
    "        md.write(\"# Tables\\n\")\n",
    "        for i, table_file in enumerate(table_files):\n",
    "            df = pd.read_csv(table_file)\n",
    "            md.write(f\"## Table {i + 1}\\n\")\n",
    "            md.write(df.to_markdown(index=False))\n",
    "            md.write(\"\\n\\n\")\n",
    "    print(f\"Markdown file saved: {output_file}\")\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.geeksforgeeks.org/python-programming-language-tutorial/\"\n",
    "\n",
    "    # Scrape data\n",
    "    print(\"Scraping images...\")\n",
    "    image_urls = scrape_images(url)\n",
    "\n",
    "    print(\"Scraping tables...\")\n",
    "    table_files = scrape_tables(url)\n",
    "\n",
    "    print(\"Scraping metadata...\")\n",
    "    metadata_file = scrape_metadata(url)\n",
    "\n",
    "    # Generate and save Markdown file\n",
    "    print(\"Creating final markdown file...\")\n",
    "    save_markdown(metadata_file, table_files, image_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c69caf-e3d1-47f6-8577-bae3e1ec35db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
