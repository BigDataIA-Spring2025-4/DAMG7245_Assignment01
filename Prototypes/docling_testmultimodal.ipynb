{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/damg7245/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.utils.export import generate_multimodal_pages\n",
    "from docling.utils.utils import create_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_RESOLUTION_SCALE = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    input_doc_path = Path(\"Samples/wikitest1.pdf\")\n",
    "    output_dir = Path(\"Samples\")\n",
    "\n",
    "    # Important: For operating with page images, we must keep them, otherwise the DocumentConverter\n",
    "    # will destroy them for cleaning up memory.\n",
    "    # This is done by setting AssembleOptions.images_scale, which also defines the scale of images.\n",
    "    # scale=1 correspond of a standard 72 DPI image\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "    pipeline_options.generate_page_images = True\n",
    "\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    conv_res = doc_converter.convert(input_doc_path)\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rows = []\n",
    "    for (\n",
    "        content_text,\n",
    "        content_md,\n",
    "        content_dt,\n",
    "        page_cells,\n",
    "        page_segments,\n",
    "        page,\n",
    "    ) in generate_multimodal_pages(conv_res):\n",
    "\n",
    "        dpi = page._default_image_scale * 72\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"document\": conv_res.input.file.name,\n",
    "                \"hash\": conv_res.input.document_hash,\n",
    "                \"page_hash\": create_hash(\n",
    "                    conv_res.input.document_hash + \":\" + str(page.page_no - 1)\n",
    "                ),\n",
    "                \"image\": {\n",
    "                    \"width\": page.image.width,\n",
    "                    \"height\": page.image.height,\n",
    "                    \"bytes\": page.image.tobytes(),\n",
    "                },\n",
    "                \"cells\": page_cells,\n",
    "                \"contents\": content_text,\n",
    "                \"contents_md\": content_md,\n",
    "                \"contents_dt\": content_dt,\n",
    "                \"segments\": page_segments,\n",
    "                \"extra\": {\n",
    "                    \"page_num\": page.page_no + 1,\n",
    "                    \"width_in_points\": page.size.width,\n",
    "                    \"height_in_points\": page.size.height,\n",
    "                    \"dpi\": dpi,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Generate one parquet from all documents\n",
    "    df = pd.json_normalize(rows)\n",
    "    now = datetime.datetime.now()\n",
    "    output_filename = output_dir / f\"multimodal_{now:%Y-%m-%d_%H%M%S}.parquet\"\n",
    "    print(df)\n",
    "    df.to_markdown()\n",
    "    df.to_parquet(output_filename)\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    _log.info(\n",
    "        f\"Document converted and multimodal pages generated in {end_time:.2f} seconds.\"\n",
    "    )\n",
    "\n",
    "    # This block demonstrates how the file can be opened with the HF datasets library\n",
    "    from datasets import Dataset\n",
    "    from PIL import Image\n",
    "    multimodal_df = pd.read_parquet(output_filename)\n",
    "    multimodal_df.head()\n",
    "\n",
    "    # # Convert pandas DataFrame to Hugging Face Dataset and load bytes into image\n",
    "    # dataset = Dataset.from_pandas(multimodal_df)\n",
    "    # def transforms(examples):\n",
    "    #     examples[\"image\"] = Image.frombytes('RGB', (examples[\"image.width\"], examples[\"image.height\"]), examples[\"image.bytes\"], 'raw')\n",
    "    #     return examples\n",
    "    # dataset = dataset.map(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'mps'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'mps'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'mps'\n",
      "INFO:docling.pipeline.base_pipeline:Processing document wikitest1.pdf\n",
      "INFO:docling.document_converter:Finished converting document wikitest1.pdf in 69.43 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         document                                               hash  \\\n",
      "0   wikitest1.pdf  ea3bf30568489edae008ff5786f7d4c11cc512c811e337...   \n",
      "1   wikitest1.pdf  ea3bf30568489edae008ff5786f7d4c11cc512c811e337...   \n",
      "2   wikitest1.pdf  ea3bf30568489edae008ff5786f7d4c11cc512c811e337...   \n",
      "3   wikitest1.pdf  ea3bf30568489edae008ff5786f7d4c11cc512c811e337...   \n",
      "4   wikitest1.pdf  ea3bf30568489edae008ff5786f7d4c11cc512c811e337...   \n",
      "5   wikitest1.pdf  ea3bf30568489edae008ff5786f7d4c11cc512c811e337...   \n",
      "6   wikitest1.pdf  ea3bf30568489edae008ff5786f7d4c11cc512c811e337...   \n",
      "7   wikitest1.pdf  ea3bf30568489edae008ff5786f7d4c11cc512c811e337...   \n",
      "8   wikitest1.pdf  ea3bf30568489edae008ff5786f7d4c11cc512c811e337...   \n",
      "9   wikitest1.pdf  ea3bf30568489edae008ff5786f7d4c11cc512c811e337...   \n",
      "10  wikitest1.pdf  ea3bf30568489edae008ff5786f7d4c11cc512c811e337...   \n",
      "\n",
      "                                            page_hash  \\\n",
      "0   d6363da410be8d04f87aae6a6733395ded234f1e8ba7da...   \n",
      "1   0ec4785d564c845c0a01007525ef776d8135d6c94950fe...   \n",
      "2   9f2ac3a222c3768e7159a86da40b0386ccca5e6c6c450e...   \n",
      "3   ed0c6cb37f1b54d8c8b553d544862e71bdd247d71bb04a...   \n",
      "4   384b225d337f2ac0acc263a8ae7cf97648c029db75a44e...   \n",
      "5   ecbcc67dc5a2e3fa6cbbb911d5dd88e48fb7a21a078b4f...   \n",
      "6   7fff7c2fa65411925a86cfc0c6e1292e49ee94cf13557f...   \n",
      "7   5698356734253d4fba0cd1b1cb51c40e93c968fc7be0ec...   \n",
      "8   77ae759922a07353eef8a2663ae468d900f64abab968ec...   \n",
      "9   d9b173593d748726576839d135917fc2e4a5c58bf05630...   \n",
      "10  b0c2b85ba847f4b1eb2891e270f9d20f52abbd09bd3819...   \n",
      "\n",
      "                                                cells  \\\n",
      "0   [{'text': 'The training compute of notable', '...   \n",
      "1   [{'text': 'As an example, consider a tokenizer...   \n",
      "2   [{'text': 'When each head calculates, accordin...   \n",
      "3   [{'text': 'At point(s) referred to as breaks,'...   \n",
      "4   [{'text': 'chain-of-thought prompting: Model o...   \n",
      "5   [{'text': 'In  information  theory,  the  conc...   \n",
      "6   [{'text': 'Some commenters expressed concern o...   \n",
      "7   [{'text': '13. Movva, Rajiv; Balachandar, Sidh...   \n",
      "8   [{'text': '48. Zaib, Munazza; Sheng, Quan Z.; ...   \n",
      "9   [{'text': '84. Alayrac, Jean-Baptiste; Donahue...   \n",
      "10  [{'text': '123. Wayne Xin Zhao; Zhou, Kun; Li,...   \n",
      "\n",
      "                                             contents  \\\n",
      "0   1/22/25, 3:29 PM Large language model - Wikipe...   \n",
      "1   1/22/25, 3:29 PM Large language model - Wikipe...   \n",
      "2   1/22/25, 3:29 PM Large language model - Wikipe...   \n",
      "3   1/22/25, 3:29 PM Large language model - Wikipe...   \n",
      "4   1/22/25, 3:29 PM Large language model - Wikipe...   \n",
      "5   1/22/25, 3:29 PM Large language model - Wikipe...   \n",
      "6   1/22/25, 3:29 PM Large language model - Wikipe...   \n",
      "7   1/22/25, 3:29 PM Large language model - Wikipe...   \n",
      "8   1/22/25, 3:29 PM Large language model - Wikipe...   \n",
      "9   1/22/25, 3:29 PM Large language model - Wikipe...   \n",
      "10  1/22/25, 3:29 PM Large language model - Wikipe...   \n",
      "\n",
      "                                          contents_md  \\\n",
      "0   <!-- image -->\\n\\n## Large language model\\n\\nA...   \n",
      "1   As an example, consider a tokenizer based on b...   \n",
      "2   Models  may  be  trained  on  auxiliary  tasks...   \n",
      "3   Typically, LLMs are trained with single- or ha...   \n",
      "4   - chain-of-thought prompting: Model outputs ar...   \n",
      "5   In  information  theory,  the  concept  of  en...   \n",
      "6   Some commenters expressed concern over acciden...   \n",
      "7   - 13. Movva, Rajiv; Balachandar, Sidhika; Peng...   \n",
      "8   - 48. Zaib, Munazza; Sheng, Quan Z.; Emma Zhan...   \n",
      "9   - 84. Alayrac, Jean-Baptiste; Donahue, Jeff; L...   \n",
      "10  - 123. Wayne Xin Zhao; Zhou, Kun; Li, Junyi; T...   \n",
      "\n",
      "                                          contents_dt  \\\n",
      "0   <document>\\n<figure>\\n<location><loc_5><loc_92...   \n",
      "1   <document>\\n<paragraph><location><loc_5><loc_9...   \n",
      "2   <document>\\n<paragraph><location><loc_5><loc_9...   \n",
      "3   <document>\\n<paragraph><location><loc_5><loc_9...   \n",
      "4   <document>\\n<paragraph><location><loc_7><loc_9...   \n",
      "5   <document>\\n<paragraph><location><loc_5><loc_9...   \n",
      "6   <document>\\n<paragraph><location><loc_5><loc_9...   \n",
      "7   <document>\\n<paragraph><location><loc_5><loc_8...   \n",
      "8   <document>\\n<paragraph><location><loc_5><loc_8...   \n",
      "9   <document>\\n<paragraph><location><loc_5><loc_8...   \n",
      "10  <document>\\n<paragraph><location><loc_5><loc_9...   \n",
      "\n",
      "                                             segments  image.width  \\\n",
      "0   [{'index_in_doc': 0, 'label': 'page_header', '...         1224   \n",
      "1   [{'index_in_doc': 30, 'label': 'page_header', ...         1224   \n",
      "2   [{'index_in_doc': 59, 'label': 'page_header', ...         1224   \n",
      "3   [{'index_in_doc': 86, 'label': 'page_header', ...         1224   \n",
      "4   [{'index_in_doc': 120, 'label': 'page_header',...         1224   \n",
      "5   [{'index_in_doc': 146, 'label': 'page_header',...         1224   \n",
      "6   [{'index_in_doc': 175, 'label': 'page_header',...         1224   \n",
      "7   [{'index_in_doc': 206, 'label': 'page_header',...         1224   \n",
      "8   [{'index_in_doc': 245, 'label': 'page_header',...         1224   \n",
      "9   [{'index_in_doc': 285, 'label': 'page_header',...         1224   \n",
      "10  [{'index_in_doc': 328, 'label': 'page_header',...         1224   \n",
      "\n",
      "    image.height                                        image.bytes  \\\n",
      "0           1584  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...   \n",
      "1           1584  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...   \n",
      "2           1584  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...   \n",
      "3           1584  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...   \n",
      "4           1584  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...   \n",
      "5           1584  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...   \n",
      "6           1584  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...   \n",
      "7           1584  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...   \n",
      "8           1584  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...   \n",
      "9           1584  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...   \n",
      "10          1584  b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff...   \n",
      "\n",
      "    extra.page_num  extra.width_in_points  extra.height_in_points  extra.dpi  \n",
      "0                1                  612.0                   792.0      144.0  \n",
      "1                2                  612.0                   792.0      144.0  \n",
      "2                3                  612.0                   792.0      144.0  \n",
      "3                4                  612.0                   792.0      144.0  \n",
      "4                5                  612.0                   792.0      144.0  \n",
      "5                6                  612.0                   792.0      144.0  \n",
      "6                7                  612.0                   792.0      144.0  \n",
      "7                8                  612.0                   792.0      144.0  \n",
      "8                9                  612.0                   792.0      144.0  \n",
      "9               10                  612.0                   792.0      144.0  \n",
      "10              11                  612.0                   792.0      144.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Document converted and multimodal pages generated in 70.35 seconds.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "damg7245",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
